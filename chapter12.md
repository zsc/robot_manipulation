# 第12章：3D感知与场景理解

本章深入探讨机器人3D感知技术，从传感器原理到高级场景理解算法。我们将学习如何从原始传感器数据构建完整的3D环境模型，实现物体识别、位姿估计和语义理解。这些技术是机器人自主操作和导航的基础，直接影响任务执行的成功率。

## 12.1 点云获取：LiDAR vs ToF vs 结构光

### 12.1.1 LiDAR（激光雷达）

LiDAR通过测量激光脉冲的飞行时间(Time of Flight)获取距离信息。机械式LiDAR通过旋转镜面扫描环境，固态LiDAR使用相控阵或MEMS技术实现电子扫描。

**工作原理**：
$$d = \frac{c \cdot \Delta t}{2}$$

其中$c$为光速，$\Delta t$为激光往返时间。

**关键参数**：
- **角分辨率**：水平0.1°-0.4°，垂直0.2°-2°
- **测距精度**：±2-5cm（30m处）
- **最大测程**：100-300m（反射率>10%）
- **点云密度**：300k-2M点/秒
- **视场角**：水平360°，垂直30°-40°

**优势**：
- 长测程，高精度
- 不受环境光影响
- 可获取反射强度信息

**劣势**：
- 成本高（机械式$5k-50k）
- 机械部件易损（MTBF ~5000小时）
- 雨雾天性能下降
- 对透明/高反射表面失效

### 12.1.2 ToF相机（飞行时间相机）

ToF相机使用调制光源和相位检测原理，通过测量反射光的相位差计算距离。

**连续波调制原理**：
$$\phi = 2 \pi f_m \cdot \frac{2d}{c}$$
$$d = \frac{c \cdot \phi}{4 \pi f_m}$$

其中$f_m$为调制频率（通常20-100MHz），$\phi$为相位差。

**关键参数**：
- **分辨率**：QVGA(320×240) - VGA(640×480)
- **帧率**：30-60 FPS
- **测距精度**：1%距离（标准偏差）
- **测程**：0.5-10m（室内），可扩展至30m
- **视场角**：60°-90°

**多路径干扰问题**：
当光线经多次反射返回时，测量相位为多条路径的加权和：
$$I_{measured} = \sum_i A_i \cdot e^{j\phi_i}$$

这导致凹角处产生"飞点"(flying pixels)。

### 12.1.3 结构光

结构光通过投影已知图案并分析变形来计算深度。

**三角测量原理**：
$$Z = \frac{f \cdot b}{d}$$

其中$f$为焦距，$b$为基线长度，$d$为视差。

**编码策略**：
1. **时间编码**：格雷码、相移法
2. **空间编码**：De Bruijn序列、伪随机点阵
3. **混合编码**：结合时间和空间信息

**散斑结构光**（如Kinect v1、iPhone FaceID）：
- 投射伪随机红外点阵（~30k点）
- 通过块匹配计算视差
- 深度分辨率：$\delta Z = \frac{Z^2}{f \cdot b} \cdot \delta d$

**关键参数对比**：

| 特性 | LiDAR | ToF相机 | 结构光 |
|------|--------|---------|---------|
| 测程 | 100-300m | 0.5-10m | 0.3-5m |
| 精度 | ±2-5cm | 1%距离 | 0.1%距离 |
| 分辨率 | 稀疏点云 | VGA级别 | VGA-1080p |
| 帧率 | 10-20Hz | 30-60Hz | 30Hz |
| 室外性能 | 优秀 | 差 | 极差 |
| 成本 | 高($5k+) | 中($500) | 低($50) |

### 12.1.4 传感器标定与同步

深度传感器的精确标定是实现高质量3D感知的前提。标定误差直接传播到后续的所有处理步骤，因此需要特别重视。

**内参标定**（ToF/结构光相机）：

内参矩阵描述了3D点到像素坐标的投影关系：
$$\begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = \begin{bmatrix} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X/Z \\ Y/Z \\ 1 \end{bmatrix}$$

其中$(f_x, f_y)$为焦距，$(c_x, c_y)$为主点。对于深度相机，还需要标定深度畸变模型：
$$d_{corrected} = d_{raw} \cdot (1 + k_1r^2 + k_2r^4) + k_3$$

标定过程通常使用棋盘格或AprilTag，采集20-50张不同姿态的图像。Zhang's方法通过最小化重投影误差求解参数，典型精度可达0.3像素RMS。

**外参标定**（多传感器融合）：

外参描述传感器间的刚体变换关系。对于LiDAR-相机标定，常用方法包括：

1. **基于标定板**：使用带有反射标记的标定板，同时在LiDAR和相机中检测
2. **基于自然特征**：利用环境中的边缘、平面等几何特征
3. **运动标定**：通过传感器运动轨迹的一致性约束求解

优化目标函数：
$$E = \sum_i \|p_i - \pi(T \cdot P_i)\|^2 + \lambda \sum_j \|n_j^T(T \cdot Q_j - q_j)\|^2$$

第一项为点重投影误差，第二项为平面约束。使用Levenberg-Marquardt算法迭代优化。

**时间同步**：

多传感器系统的时间同步至关重要，特别是在动态场景中。不同步会导致点云畸变和配准失败。

- **硬件触发**：通过GPIO或专用触发线实现，精度<1μs
  - 优点：精度最高，适合高速运动
  - 缺点：需要硬件支持，布线复杂

- **PTP协议**（IEEE 1588）：网络时间同步，精度<1ms
  - 适用于支持PTP的工业相机和LiDAR
  - 需要PTP主时钟和交换机支持

- **软件时间戳**：基于系统时钟，精度~10ms
  - 需要估计和补偿传输延迟
  - 可使用NTP或chrony进行粗同步

**时间偏移估计**：
对于软同步系统，可通过互相关分析估计时间偏移：
$$\tau^* = \arg\max_\tau \sum_t f_1(t) \cdot f_2(t + \tau)$$

其中$f_1$、$f_2$为两个传感器的特征时间序列（如检测到的特征点数量）。

### 12.1.5 传感器选择策略

根据应用场景选择合适的深度传感器是系统设计的关键决策。以下是典型应用场景的推荐：

**室内导航与建图**：
- 首选：固态LiDAR（如Livox Mid-360）
- 备选：深度相机阵列（多视角覆盖）
- 关键指标：视场角>270°，更新率>10Hz

**机械臂抓取**：
- 首选：结构光相机（如Intel RealSense D435）
- 备选：ToF相机（对透明物体）
- 关键指标：精度<1mm@1m，分辨率>VGA

**室外自主驾驶**：
- 主传感器：机械式LiDAR（如Velodyne VLS-128）
- 辅助：长距ToF + 毫米波雷达
- 关键指标：测程>200m，360°覆盖

**成本约束项目**：
- 单目+IMU+深度估计网络
- 立体视觉（计算负载较高）
- RGB-D相机（如OAK-D）

**传感器冗余设计**：
在安全关键应用中，需要考虑传感器失效模式：
- 异构冗余：不同原理的传感器互补
- 同构冗余：相同传感器的表决机制
- 优雅降级：单传感器失效时的应急策略

## 12.2 点云处理：滤波、配准、分割

原始点云数据通常包含噪声、离群点和冗余信息。有效的预处理是后续算法成功的基础。本节介绍点云处理的核心算法，重点关注实时性和鲁棒性的平衡。

### 12.2.1 点云滤波

**体素格栅滤波（Voxel Grid Filter）**：
将空间划分为立方体网格，每个体素保留一个代表点（质心或随机）。

```
对于体素大小 r：
1. 计算每个点的体素索引：(⌊x/r⌋, ⌊y/r⌋, ⌊z/r⌋)
2. 对同一体素内的点求质心
3. 时间复杂度：O(n)，空间复杂度：O(m)，m为体素数
```

**统计滤波（Statistical Outlier Removal）**：
基于邻域点距离的统计分布去除离群点。

对每个点$p_i$：
1. 计算k近邻平均距离：$\bar{d}_i = \frac{1}{k}\sum_{j \in N_k(i)} \|p_i - p_j\|$
2. 计算全局均值和标准差：$\mu = \frac{1}{n}\sum_i \bar{d}_i$，$\sigma = \sqrt{\frac{1}{n}\sum_i (\bar{d}_i - \mu)^2}$
3. 剔除条件：$\bar{d}_i > \mu + \alpha \cdot \sigma$（通常$\alpha = 2$）

**半径滤波（Radius Outlier Filter）**：
剔除指定半径内邻居数量小于阈值的点。适用于去除稀疏噪声。

**双边滤波（Bilateral Filter）**：
保边平滑，同时考虑空间距离和法向量相似度：
$$p'_i = \frac{\sum_{j \in N(i)} w_s(\|p_i - p_j\|) \cdot w_r(\|n_i - n_j\|) \cdot p_j}{\sum_{j \in N(i)} w_s(\|p_i - p_j\|) \cdot w_r(\|n_i - n_j\|)}$$

其中$w_s$、$w_r$为高斯核函数。

### 12.2.2 点云配准

**ICP（Iterative Closest Point）算法**：

基本ICP迭代步骤：
1. **对应点查找**：$c_i = \arg\min_{q \in Q} \|T \cdot p_i - q\|$
2. **变换估计**：$T^* = \arg\min_T \sum_i \|T \cdot p_i - c_i\|^2$
3. **收敛判断**：$\Delta E < \epsilon$ 或 迭代次数达到上限

**点到点ICP优化问题**：
$$E_{p2p} = \sum_{i=1}^n \|R \cdot p_i + t - q_i\|^2$$

闭式解（SVD方法）：
1. 计算质心：$\bar{p} = \frac{1}{n}\sum p_i$，$\bar{q} = \frac{1}{n}\sum q_i$
2. 去中心化：$p'_i = p_i - \bar{p}$，$q'_i = q_i - \bar{q}$
3. 协方差矩阵：$H = \sum p'_i \cdot q'^T_i$
4. SVD分解：$H = U\Sigma V^T$
5. 旋转矩阵：$R = VU^T$（处理行列式为-1的情况）
6. 平移向量：$t = \bar{q} - R\bar{p}$

**点到面ICP（Point-to-Plane）**：
$$E_{p2plane} = \sum_{i=1}^n ((R \cdot p_i + t - q_i) \cdot n_i)^2$$

使用Gauss-Newton迭代求解，收敛速度更快。

**GICP（Generalized ICP）**：
将点云不确定性建模为协方差矩阵：
$$E_{GICP} = \sum_{i=1}^n (p_i - Tq_i)^T(C_i^p + TC_i^qT^T)^{-1}(p_i - Tq_i)$$

**NDT（Normal Distribution Transform）**：
1. 将空间划分为网格
2. 每个网格内的点拟合高斯分布$N(\mu, \Sigma)$
3. 最大化点云与NDT场的匹配概率

**配准初始化方法**：
- **FPFH特征**：基于点的邻域几何特征
- **4PCS算法**：随机采样共面四点组
- **语义特征**：利用物体类别信息约束

### 12.2.3 点云分割

**平面分割（RANSAC）**：
```
输入：点云P，迭代次数K，内点阈值d
1. for k = 1 to K:
2.   随机采样3个点，拟合平面：ax + by + cz + d = 0
3.   计算所有点到平面距离
4.   统计内点数量（距离<d）
5.   保留最大内点集合
6. 最小二乘重新拟合最终平面
```

迭代次数估计：$K = \frac{\log(1-p)}{\log(1-w^n)}$
其中$p$为成功概率，$w$为内点比例，$n$为最小采样数。

**欧式聚类（Euclidean Clustering）**：
基于点间距离的区域生长算法：
```
1. 构建KD-Tree加速近邻搜索
2. for 每个未访问点p:
3.   创建新簇C，将p加入C和队列Q
4.   while Q非空:
5.     取出点q，标记为已访问
6.     找出q的r-邻域内所有点
7.     将未访问的邻域点加入C和Q
```

**区域生长（Region Growing）**：
基于法向量和曲率的生长：
1. 计算所有点的法向量和曲率
2. 从曲率最小的点开始生长
3. 生长条件：法向量夹角<θ_n且曲率差<θ_c

**基于图的分割（Graph-based）**：
构建图$G=(V,E)$，顶点为超体素，边权重为相似度。使用归一化割（Normalized Cut）优化：
$$NCut(A,B) = \frac{cut(A,B)}{assoc(A,V)} + \frac{cut(A,B)}{assoc(B,V)}$$

**深度学习方法**：

传统方法在复杂场景下往往失效，深度学习提供了端到端的解决方案：

- **PointNet++**：层次化特征学习
  - 使用集合抽象（Set Abstraction）模块提取局部特征
  - 采样策略：FPS（最远点采样）保证覆盖
  - 分组策略：Ball Query或K-NN
  - 特征聚合：max pooling保持排列不变性

- **DGCNN**（Dynamic Graph CNN）：
  - 动态构建k-NN图，捕获局部几何结构
  - EdgeConv操作：$x'_i = \max_{j \in N(i)} h_\theta(x_i, x_j - x_i)$
  - 多尺度特征融合提升鲁棒性

- **MinkowskiNet**：稀疏3D卷积
  - 将点云体素化为稀疏张量
  - 仅在非空体素上执行卷积
  - 计算效率比密集卷积高10-100倍
  - 适合大规模场景分割

- **Transformer方法**（Point Transformer）：
  - 自注意力机制捕获长程依赖
  - 位置编码保留空间信息
  - 计算复杂度$O(n^2)$，需要采样策略

### 12.2.4 点云法向量估计

法向量是描述局部几何的关键特征，用于配准、分割和重建。

**PCA方法**：
对点$p_i$的k-邻域进行主成分分析：
1. 计算协方差矩阵：$C = \frac{1}{k}\sum_{j \in N_k(i)}(p_j - \bar{p})(p_j - \bar{p})^T$
2. 特征值分解：$C = V\Lambda V^T$
3. 最小特征值对应的特征向量为法向量
4. 曲率估计：$\kappa = \frac{\lambda_0}{\lambda_0 + \lambda_1 + \lambda_2}$

**法向量定向**：
法向量具有二义性（±n都valid），需要一致定向：
1. **视点约束**：$n \cdot (v - p) > 0$，其中$v$为视点
2. **最小生成树**：构建kNN图，传播法向量方向
3. **Poisson方程**：求解调和函数的梯度场

**鲁棒估计**：
对于噪声数据，使用RANSAC或加权最小二乘：
$$C_{robust} = \sum_{j \in N_k(i)} w_j(p_j - \bar{p})(p_j - \bar{p})^T$$

权重$w_j$基于距离或法向量一致性。

### 12.2.5 点云特征描述子

特征描述子用于点云匹配和识别，需要对旋转、噪声和遮挡鲁棒。

**FPFH（Fast Point Feature Histograms）**：
1. 计算点对$(p_i, p_j)$的Darboux坐标系
2. 计算角度特征：$(\alpha, \phi, \theta)$
3. 构建33维直方图（11 bins × 3 angles）
4. 简化版本加速计算：SPFH + 邻域加权

**SHOT（Signature of Histograms of Orientations）**：
- 构建局部参考框架（唯一且可重复）
- 将邻域划分为32个区域（2径向×2方位×8俯仰）
- 每个区域计算11维余弦直方图
- 总计352维描述子

**学习型描述子**：
- **3DMatch**：使用3D CNN学习描述子
- **PPFNet**：基于点对特征的深度网络
- **DIP**：可微分兴趣点检测器

描述子性能评估指标：
- **重复性**：相同物体不同视角下的检测一致性
- **独特性**：不同物体描述子的区分度
- **匹配精度**：正确匹配率@给定内点阈值

## 12.3 6D位姿估计：基于模型vs无模型方法

6D位姿估计（3D位置+3D旋转）是机器人抓取和操作的核心技术。准确的位姿估计直接决定了机械臂能否成功接触和操作目标物体。本节介绍主流的位姿估计方法及其在实际系统中的应用。

### 12.3.1 问题定义与挑战

**6D位姿参数化**：
- 位置：3D平移向量 $t \in \mathbb{R}^3$
- 姿态：旋转矩阵 $R \in SO(3)$、四元数 $q \in \mathbb{H}$、或欧拉角
- 完整位姿：$T = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} \in SE(3)$

**主要挑战**：
1. **对称性**：物体的几何或纹理对称导致位姿歧义
2. **遮挡**：部分可见情况下的鲁棒估计
3. **光照变化**：反光、阴影影响特征提取
4. **实时性**：机器人控制需要<100ms延迟
5. **泛化性**：对未见过的物体类别的适应能力

### 12.3.2 基于模型的方法

基于模型的方法假设已知物体的CAD模型或预先扫描的3D模型。

**特征匹配方法**：
1. 提取2D/3D特征点
2. 建立2D-3D或3D-3D对应关系
3. 使用PnP或ICP求解位姿

**PPF（Point Pair Features）**：
对于模型上的点对$(m_1, m_2)$，计算特征向量：
$$F(m_1, m_2) = (\|d\|, \angle(n_1, d), \angle(n_2, d), \angle(n_1, n_2))$$

其中$d = m_2 - m_1$，$n_i$为法向量。离线构建哈希表，在线投票确定位姿。

**模板匹配方法**：
- **LineMOD**：多模态模板（梯度+法向量）
  - 在线性化的特征空间中匹配
  - 使用spreading优化梯度方向量化
  - 实时性能：~10ms/frame

- **AAE（Augmented Autoencoder）**：
  - 训练编码器将RGB图像映射到隐空间
  - 解码器从隐码重建物体多视角
  - 测试时通过隐码检索最近邻视角

**迭代优化方法**：
从初始估计开始，最小化重投影误差：
$$E = \sum_i \rho(\|p_i - \pi(K \cdot T \cdot M_i)\|)$$

其中$\rho$为鲁棒核函数（如Huber），$K$为相机内参，$M_i$为模型点。

使用Levenberg-Marquardt或Gauss-Newton迭代优化。关键是良好的初始化和外点剔除。

### 12.3.3 无模型方法

无模型方法不需要预先的CAD模型，直接从数据学习位姿估计。

**类别级位姿估计**：
估计同类物体（如所有杯子）的标准化位姿。

- **NOCS（Normalized Object Coordinate Space）**：
  - 预测每个像素的NOCS坐标（物体标准空间）
  - 通过Umeyama算法求解相似变换
  - 同时估计尺度、旋转和平移

- **Shape Prior方法**：
  - 学习类别的形状先验（如PCA基）
  - 联合优化形状参数和位姿
  - 可处理严重遮挡情况

**关键点检测方法**：
1. 检测语义关键点（如角点、中心）
2. 使用PnP求解位姿
3. 关键点定义：
   - 几何关键点：边界框角点
   - 语义关键点：功能部件（把手、开口）

**直接回归方法**：
- **PoseCNN**：直接回归平移和旋转
  - 平移：预测物体中心+深度
  - 旋转：四元数回归（需要对称性处理）
  
- **深度学习的旋转表示**：
  - 连续6D表示：前两列正交化得到旋转矩阵
  - 避免四元数/欧拉角的不连续性
  - 损失函数：测地距离 $L = \arccos(\frac{tr(R^T\hat{R})-1}{2})$

### 12.3.4 RGB-D融合方法

结合颜色和深度信息提高估计精度。

**DenseFusion架构**：
1. 分别处理RGB和点云特征
2. 像素级特征融合（Dense fusion）
3. 迭代精化网络提升精度
4. 置信度预测用于多假设场景

**处理传感器噪声**：
- 深度图修复：双边滤波、学习型补全
- 多视角融合：贝叶斯聚合多帧估计
- 不确定性建模：预测每个估计的协方差

### 12.3.5 对称性处理

对称物体的位姿估计需要特殊处理。

**对称性分类**：
1. **离散对称**：如立方体（24个等价位姿）
2. **连续对称**：如圆柱（绕轴任意旋转）
3. **部分对称**：仅在某些视角下对称

**处理策略**：
- **多假设预测**：输出所有等价位姿
- **对称感知损失**：
  $$L_{sym} = \min_{S \in Sym(O)} d(T_{pred}, T_{gt} \cdot S)$$
  其中$Sym(O)$为物体的对称群

- **关键点对称性**：定义对称不变的关键点
- **形状空间方法**：在隐空间中学习对称等价类

### 12.3.6 实时优化策略

**级联精化**：
1. 粗估计：快速但不精确（~5ms）
2. 精细化：ICP或深度精化（~20ms）
3. 跟踪模式：利用时序信息（~2ms）

**硬件加速**：
- GPU并行：RANSAC、ICP的并行实现
- 量化部署：INT8推理加速2-4倍
- 模型剪枝：去除冗余通道

**失败检测与恢复**：
- 置信度阈值：拒绝低置信度估计
- 多视角验证：从不同角度验证一致性
- 回退策略：失败时切换到更鲁棒但较慢的方法

## 12.4 物体检测与实例分割

在复杂场景中识别和定位多个物体是机器人感知的基本能力。本节讨论3D物体检测和实例分割技术，重点关注点云和RGB-D数据的处理方法。

### 12.4.1 3D物体检测

3D物体检测输出物体的3D边界框（位置、尺寸、朝向）和类别。

**问题表述**：
- 输入：点云 $P = \{p_i\}$ 或 RGB-D图像
- 输出：边界框 $(x, y, z, w, h, l, \theta)$ + 类别标签
- 评估指标：3D IoU、Average Precision (AP)

**基于点云的方法**：

**VoxelNet架构**：
1. **体素化**：将点云划分为规则体素网格
2. **特征编码**：每个体素内的点通过PointNet编码
3. **3D卷积**：稀疏3D CNN提取特征
4. **检测头**：预测框的回归和分类

体素特征编码：
$$f_{voxel} = \max_{p_i \in V} MLP(p_i - \bar{p}, p_i)$$

**PointPillars优化**：
- 使用竖直柱体(pillars)代替3D体素
- 转换为2D伪图像，使用2D CNN
- 速度提升10倍，精度相当

**基于图像的方法**：

**Frustum PointNet**：
1. 2D检测器生成图像边界框
2. 将2D框投影为3D视锥(frustum)
3. 在视锥内的点云上进行3D检测
4. 两阶段方法，精度高但速度慢

**MonoDepth方法**：
- 从单目图像直接回归3D框
- 关键是准确的深度估计
- 使用几何约束（如地面假设）

### 12.4.2 3D实例分割

实例分割为每个物体实例生成精确的点级mask。

**自顶向下方法**：
1. 先检测3D边界框
2. 在框内进行点分类
3. 后处理去除重叠

**自底向上方法**：

**SGPN (Similarity Group Proposal Network)**：
- 学习点对相似度矩阵
- 通过谱聚类生成实例
- 端到端可训练

相似度学习：
$$S_{ij} = MLP(f_i, f_j, p_i - p_j)$$

**ASIS (Associatively Segmenting Instances)**：
- 联合学习语义和实例嵌入
- 语义分支提供类别先验
- 实例分支学习判别性特征

损失函数：
$$L = L_{sem} + \lambda_1 L_{ins} + \lambda_2 L_{assoc}$$

其中$L_{assoc}$强制同实例点的特征相近。

**基于投票的方法**：

**VoteNet**：
- 每个点投票给物体中心
- Hough投票在深度特征空间
- 聚类投票生成物体提议

投票生成：
$$v_i = p_i + \Delta_i$$
其中$\Delta_i = MLP(f_i)$预测到中心的偏移。

### 12.4.3 Transformer方法

Transformer架构在3D感知中展现出强大能力。

**3DETR架构**：
- 无需手工设计的anchor或NMS
- 全局注意力捕获长程依赖
- 二分图匹配训练

查询机制：
- 学习型物体查询(object queries)
- 每个查询负责检测一个物体
- 通过注意力聚合全局信息

**Group-Free 3D**：
- 直接从点云预测物体
- 无需分组或聚类
- 查询点通过注意力聚合特征

### 12.4.4 多模态融合

结合多种传感器模态提升检测性能。

**早期融合**：
- 将RGB特征投影到点云
- 点云增强：$p'_i = [p_i, f_{rgb}(\pi(p_i))]$
- 简单有效但丢失图像分辨率

**晚期融合**：
- 分别处理各模态
- 在决策层融合结果
- 可处理模态缺失

**深度融合**：

**MVX-Net架构**：
- PointFusion：点级特征融合
- VoxelFusion：体素级特征融合
- 自适应融合权重

融合操作：
$$f_{fused} = \alpha \cdot f_{point} + (1-\alpha) \cdot f_{image}$$

其中$\alpha$通过注意力机制学习。

### 12.4.5 弱监督与自监督

减少对精确3D标注的依赖。

**弱监督方法**：
- 使用2D框监督3D检测
- 点云配准生成伪标签
- 知识蒸馏从大模型

**自监督预训练**：
- 点云补全：遮挡部分点预测完整形状
- 对比学习：不同视角的一致性
- 场景流估计：时序点云对应

对比损失：
$$L_{contrast} = -\log \frac{\exp(sim(z_i, z_j)/\tau)}{\sum_k \exp(sim(z_i, z_k)/\tau)}$$

### 12.4.6 实时优化

**模型加速**：
- 知识蒸馏：大模型指导小模型
- 量化感知训练：INT8部署
- 结构化剪枝：去除冗余计算

**系统优化**：
- 流水线并行：检测与跟踪并行
- 区域提议共享：多任务复用
- 增量更新：只处理变化区域

**边缘部署考虑**：
- 内存占用：<2GB for Jetson
- 功耗限制：<10W for 机器人
- 延迟要求：<50ms for 30Hz
