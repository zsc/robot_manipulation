<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="./">
    <title>第15章：行为克隆与模仿学习</title>
    <link rel="stylesheet" href="assets/style.css">
    <link rel="stylesheet" href="assets/highlight.css">
    <script src="assets/script.js" defer></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$']],
                displayMath: [['$$', '$$']],
                processEscapes: false,
                packages: {'[+]': ['noerrors', 'ams']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            },
            loader: {
                load: ['[tex]/noerrors', '[tex]/ams']
            }
        };
    </script>
</head>
<body>
    <div class="container">
        <nav id="sidebar" class="sidebar">
            <div class="sidebar-header">
                <h3>目录</h3>
                <button id="sidebar-toggle" class="sidebar-toggle">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
            <div class="sidebar-search">
                <input type="text" id="sidebar-search-input" placeholder="搜索..." autocomplete="off">
            </div>
            <div id="tree-container">
                <nav class="tree-nav" role="tree">
                    <div class="tree-item " >
                        <a href="index.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">轮足机械臂机器人：从硬件制造到智能算法</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter1.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第1章：轮足机械臂架构概述</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter2.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第2章：执行器选择与优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter3.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第3章：机械结构与刚度分析</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter4.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第4章：传感器系统与数据融合</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter5.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第5章：坐标系与姿态表示</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter6.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第6章：正逆运动学与工作空间</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter7.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第7章：动力学建模与参数辨识</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter8.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第8章：轨迹规划与优化</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter9.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第9章：全身控制与平衡</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter10.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第10章：阻抗控制与力控制</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter11.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第11章：视觉感知基础</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter12.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第12章：3D感知与场景理解</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter13.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第13章：抓取理论与规划</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter14.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第14章：灵巧操作与双臂协调</span>
                        </a>
                    </div>
                
                    <div class="tree-item active" >
                        <a href="chapter15.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第15章：行为克隆与模仿学习</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter16.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第16章：扩散模型在机器人中的应用</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter17.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第17章：视觉-语言基础模型</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter18.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第18章：视觉-语言-动作模型(VLA)</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter19.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第19章：世界模型基础</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter20.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第20章：基于模型的规划与控制</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter21.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第21章：系统集成与部署</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="chapter22.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">第22章：计算平台与操作系统</span>
                        </a>
                    </div>
                
                    <div class="tree-item " >
                        <a href="CLAUDE.html" class="tree-link">
                            <span class="tree-icon">📄</span>
                            <span class="tree-title">Untitled</span>
                        </a>
                    </div>
                </nav>
            </div>
        </nav>
        
        <main class="content">
            <article>
                <h1 id="15">第15章：行为克隆与模仿学习</h1>
<h2 id="_1">章节大纲</h2>
<ol>
<li>开篇段落</li>
<li>行为克隆基础：监督学习方法
   - 从演示到策略的映射
   - 损失函数设计
   - 网络架构选择</li>
<li>数据集收集：遥操作vs演示学习
   - 遥操作系统设计
   - 人类演示数据采集
   - 数据增强技术</li>
<li>分布偏移问题与DAgger算法
   - 协变量偏移的本质
   - DAgger算法原理
   - 实践中的改进</li>
<li>逆强化学习(IRL)原理
   - 奖励函数学习
   - 最大熵IRL
   - 深度IRL方法</li>
<li>GAIL与对抗模仿学习
   - 生成对抗框架
   - 判别器与生成器设计
   - 训练稳定性技巧</li>
<li>案例研究：特斯拉FSD的模仿学习架构</li>
<li>高级话题：离线强化学习与保守Q学习(CQL)</li>
<li>本章小结</li>
<li>练习题</li>
<li>常见陷阱与错误</li>
<li>最佳实践检查清单</li>
</ol>
<hr />
<h2 id="_2">开篇段落</h2>
<p>模仿学习是机器人获得复杂技能的重要途径，它通过学习专家演示来绕过困难的奖励函数设计问题。本章深入探讨从简单的行为克隆到复杂的逆强化学习的各种方法，重点关注实际部署中的分布偏移问题及其解决方案。我们将学习如何从人类演示中提取有效策略，理解不同数据收集方法的权衡，以及如何在真实机器人系统中实现稳定的模仿学习。通过特斯拉FSD的案例分析，我们将看到这些技术如何在工业级系统中落地。</p>
<p>学习目标：</p>
<ul>
<li>掌握行为克隆的基本原理和实现细节</li>
<li>理解分布偏移问题的本质及DAgger等解决方案</li>
<li>学会设计高效的数据收集系统</li>
<li>了解IRL和GAIL等高级方法的原理与应用</li>
<li>能够在实际机器人项目中选择和实现合适的模仿学习方法</li>
</ul>
<hr />
<h2 id="1">1. 行为克隆基础：监督学习方法</h2>
<h3 id="11">1.1 从演示到策略的映射</h3>
<p>行为克隆将模仿学习问题转化为监督学习问题。给定专家演示数据集 $\mathcal{D} = \{(s_t, a_t)\}_{t=1}^N$，其中 $s_t$ 是状态，$a_t$ 是专家动作，我们的目标是学习策略 $\pi_\theta(a|s)$ 来最小化：</p>
<p>$$\mathcal{L}(\theta) = \mathbb{E}_{(s,a) \sim \mathcal{D}}[\ell(\pi_\theta(s), a)]$$
其中 $\ell$ 是损失函数。这种方法的核心假设是：如果策略能够准确预测专家在每个状态下的动作，那么执行这个策略就能复现专家的行为。</p>
<p>然而，这个假设在实践中存在重要限制：</p>
<ol>
<li><strong>因果混淆</strong>：网络可能学习到虚假的相关性</li>
<li><strong>时序依赖</strong>：单步预测忽略了动作序列的时序结构</li>
<li><strong>多模态性</strong>：专家可能在相同状态下采取不同的有效动作</li>
</ol>
<h3 id="12">1.2 损失函数设计</h3>
<p>损失函数的选择直接影响学习效果：</p>
<p><strong>连续动作空间</strong>：</p>
<ul>
<li><strong>MSE损失</strong>：$\ell_{MSE} = ||a - \hat{a}||^2$</li>
<li>优点：梯度稳定，优化简单</li>
<li>
<p>缺点：对异常值敏感，假设高斯噪声</p>
</li>
<li>
<p><strong>Huber损失</strong>：
$$\ell_{Huber} = \begin{cases}
  \frac{1}{2}(a - \hat{a})^2 &amp; \text{if } |a - \hat{a}| \leq \delta \\
  \delta(|a - \hat{a}| - \frac{1}{2}\delta) &amp; \text{otherwise}
  \end{cases}$$</p>
</li>
<li>
<p>结合了MSE和MAE的优点，对异常值更鲁棒</p>
</li>
<li>
<p><strong>混合密度网络(MDN)</strong>：
$$p(a|s) = \sum_{i=1}^K \alpha_i(s) \mathcal{N}(a; \mu_i(s), \Sigma_i(s))$$</p>
</li>
<li>
<p>能够建模多模态分布，特别适合存在多个合理动作的场景</p>
</li>
</ul>
<p><strong>离散动作空间</strong>：</p>
<ul>
<li><strong>交叉熵损失</strong>：$\ell_{CE} = -\sum_i a_i \log \hat{a}_i$</li>
<li><strong>焦点损失(Focal Loss)</strong>：处理类别不平衡问题</li>
</ul>
<h3 id="13">1.3 网络架构选择</h3>
<p>架构设计需要考虑输入模态和任务特性：</p>
<p><strong>视觉输入处理</strong>：</p>
<div class="codehilite"><pre><span></span><code>图像输入 → CNN特征提取器 → 
         ↓
    空间注意力机制
         ↓
    特征融合层 → 策略头
</code></pre></div>

<p><strong>多模态融合架构</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="err">视觉流</span><span class="o">:</span><span class="w"> </span><span class="n">ResNet</span><span class="o">/</span><span class="n">ViT</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="err">视觉特征</span>
<span class="w">                    </span><span class="err">↘</span>
<span class="w">                     </span><span class="err">融合模块</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="err">共享表示</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="err">动作预测</span>
<span class="w">                    </span><span class="err">↗</span>
<span class="err">本体感知流</span><span class="o">:</span><span class="w"> </span><span class="n">MLP</span><span class="w"> </span><span class="err">→</span><span class="w"> </span><span class="err">本体特征</span>
</code></pre></div>

<p><strong>时序建模</strong>：</p>
<ul>
<li><strong>因果Transformer</strong>：处理长期依赖，支持变长历史</li>
<li><strong>LSTM/GRU</strong>：计算效率高，适合实时系统</li>
<li><strong>时序卷积网络(TCN)</strong>：并行化好，感受野可控</li>
</ul>
<hr />
<h2 id="2-vs">2. 数据集收集：遥操作vs演示学习</h2>
<h3 id="21">2.1 遥操作系统设计</h3>
<p>遥操作是收集机器人演示数据的主要方法之一。设计良好的遥操作系统需要在易用性、精度和数据质量之间取得平衡。</p>
<p><strong>遥操作接口类型</strong>：</p>
<ol>
<li>
<p><strong>直接控制接口</strong>：
   - 操纵杆/手柄：适合移动平台，但精度有限
   - 6D鼠标(SpaceMouse)：提供6自由度控制，适合精细操作
   - 触觉设备(Phantom Omni/Touch)：提供力反馈，增强操作感知</p>
</li>
<li>
<p><strong>主从式遥操作</strong>：</p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code>主臂(人操作) → 运动捕捉 → 运动重定向 → 从臂(机器人)
                           ↓
                      数据记录系统
</code></pre></div>

<ul>
<li>优点：直观自然，可以利用人的本体感知</li>
<li>挑战：工作空间映射、动力学差异补偿</li>
</ul>
<ol start="3">
<li><strong>虚拟现实(VR)遥操作</strong>：
   - 使用VR头显和手柄进行沉浸式控制
   - 视角选择：第一人称vs第三人称vs混合视角
   - 延迟补偿：预测渲染减少控制延迟感</li>
</ol>
<p><strong>数据同步与时间戳</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 伪代码：多模态数据同步</span>
<span class="k">class</span> <span class="nc">DataRecorder</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_offset</span> <span class="o">=</span> <span class="n">calibrate_clocks</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">CircularBuffer</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">record_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">get_synchronized_time</span><span class="p">()</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;timestamp&#39;</span><span class="p">:</span> <span class="n">t</span><span class="p">,</span>
            <span class="s1">&#39;rgb&#39;</span><span class="p">:</span> <span class="n">camera</span><span class="o">.</span><span class="n">get_frame</span><span class="p">(</span><span class="n">t</span><span class="p">),</span>
            <span class="s1">&#39;depth&#39;</span><span class="p">:</span> <span class="n">depth_camera</span><span class="o">.</span><span class="n">get_frame</span><span class="p">(</span><span class="n">t</span><span class="p">),</span>
            <span class="s1">&#39;joint_pos&#39;</span><span class="p">:</span> <span class="n">robot</span><span class="o">.</span><span class="n">get_joints</span><span class="p">(</span><span class="n">t</span><span class="p">),</span>
            <span class="s1">&#39;joint_vel&#39;</span><span class="p">:</span> <span class="n">robot</span><span class="o">.</span><span class="n">get_velocities</span><span class="p">(</span><span class="n">t</span><span class="p">),</span>
            <span class="s1">&#39;ee_pose&#39;</span><span class="p">:</span> <span class="n">robot</span><span class="o">.</span><span class="n">get_ee_pose</span><span class="p">(</span><span class="n">t</span><span class="p">),</span>
            <span class="s1">&#39;action&#39;</span><span class="p">:</span> <span class="n">teleop</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">t</span><span class="p">),</span>
            <span class="s1">&#39;force_torque&#39;</span><span class="p">:</span> <span class="n">ft_sensor</span><span class="o">.</span><span class="n">get_reading</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div>

<h3 id="22">2.2 人类演示数据采集</h3>
<p>直接从人类演示中学习(无需机器人硬件)有其独特优势：</p>
<p><strong>动作捕捉系统</strong>：</p>
<ul>
<li>光学系统(Vicon/OptiTrack)：高精度，但需要标记点</li>
<li>惯性系统(Xsens/Perception Neuron)：便携，但存在漂移</li>
<li>计算机视觉方法(OpenPose/MediaPipe)：低成本，但精度受限</li>
</ul>
<p><strong>从人类演示到机器人动作的映射</strong>：</p>
<ol>
<li>
<p><strong>运动重定向(Retargeting)</strong>：
$$a_{robot} = f(s_{human}, \phi_{human}, \phi_{robot})$$
其中 $\phi$ 表示形态学参数</p>
</li>
<li>
<p><strong>任务空间映射</strong>：
   - 关注末端执行器轨迹而非关节角度
   - 使用逆运动学求解机器人关节指令</p>
</li>
<li>
<p><strong>学习式映射</strong>：
   - 训练神经网络学习人到机器人的映射
   - 可以处理形态学差异和动力学约束</p>
</li>
</ol>
<h3 id="23">2.3 数据增强技术</h3>
<p>数据增强对提高策略泛化性至关重要：</p>
<p><strong>几何增强</strong>：</p>
<ul>
<li>随机裁剪和缩放：增强对物体大小变化的鲁棒性</li>
<li>视角变换：模拟不同相机位置</li>
<li>镜像翻转：适用于对称任务</li>
</ul>
<p><strong>时序增强</strong>：</p>
<ul>
<li>速度扰动：改变执行速度±20%</li>
<li>动作噪声注入：$a' = a + \epsilon, \epsilon \sim \mathcal{N}(0, \sigma^2)$</li>
<li>轨迹插值：在关键帧之间生成中间状态</li>
</ul>
<p><strong>域随机化</strong>：</p>
<div class="codehilite"><pre><span></span><code>视觉域随机化参数：

- 光照：强度[0.5, 2.0]，色温[4000K, 7000K]
- 纹理：随机替换物体和背景纹理
- 相机：内参扰动±5%，位置噪声±5cm
- 物理：摩擦系数[0.5, 1.5]，质量±20%
</code></pre></div>

<p><strong>混合增强策略</strong>：
$$\mathcal{D}_{aug} = \mathcal{D}_{orig} \cup \mathcal{T}_1(\mathcal{D}) \cup \mathcal{T}_2(\mathcal{D}) \cup ...$$
其中 $\mathcal{T}_i$ 是不同的增强变换</p>
<hr />
<h2 id="3-dagger">3. 分布偏移问题与DAgger算法</h2>
<h3 id="31">3.1 协变量偏移的本质</h3>
<p>行为克隆的核心问题是训练时和测试时的状态分布不匹配。这种分布偏移(distribution shift)会导致误差累积和性能退化。</p>
<p><strong>问题的数学描述</strong>：</p>
<ul>
<li>训练分布：$s \sim d_{\pi^*}(s)$ (专家策略诱导的状态分布)</li>
<li>测试分布：$s \sim d_{\pi_\theta}(s)$ (学习策略诱导的状态分布)</li>
<li>当 $\pi_\theta \neq \pi^*$ 时，$d_{\pi_\theta} \neq d_{\pi^*}$</li>
</ul>
<p><strong>误差累积分析</strong>：
假设单步预测误差为 $\epsilon$，在长度为 $T$ 的轨迹中：</p>
<ul>
<li>理想情况：总误差 $O(\epsilon T)$</li>
<li>实际情况：总误差 $O(\epsilon T^2)$ (由于复合误差)</li>
</ul>
<p>这种二次增长说明了为什么即使很小的预测误差也会导致长期任务失败。</p>
<p><strong>可视化分布偏移</strong>：</p>
<div class="codehilite"><pre><span></span><code>专家轨迹：  s₀ → s₁ → s₂ → s₃ → ... → sₜ
            ↓    ↓    ↓    ↓         ↓
专家动作：  a₀*  a₁*  a₂*  a₃*  ...  aₜ*

学习轨迹：  s₀ → s₁&#39; → s₂&#39; → s₃&#39; → ... → sₜ&#39;
            ↓    ↓     ↓     ↓          ↓
预测动作：  â₀   â₁    â₂    â₃   ...   âₜ

偏差累积：  0 → δ₁ → δ₁+δ₂ → δ₁+δ₂+δ₃ → ...
</code></pre></div>

<h3 id="32-dagger">3.2 DAgger算法原理</h3>
<p>Dataset Aggregation (DAgger) 通过迭代收集数据来解决分布偏移问题：</p>
<p><strong>算法流程</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="err">算法</span><span class="o">:</span><span class="w"> </span><span class="n">DAgger</span>
<span class="err">输入</span><span class="o">:</span><span class="w"> </span><span class="err">初始数据集</span><span class="w"> </span><span class="n">D₀</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">{(</span><span class="n">s</span><span class="o">,</span><span class="n">a</span><span class="o">*)}</span><span class="err">从专家收集</span>
<span class="w">      </span><span class="err">专家策略</span><span class="w"> </span><span class="err">π</span><span class="o">*</span>
<span class="w">      </span><span class="err">迭代次数</span><span class="w"> </span><span class="n">N</span>

<span class="mi">1</span><span class="o">.</span><span class="w"> </span><span class="err">在</span><span class="w"> </span><span class="n">D₀</span><span class="w"> </span><span class="err">上训练初始策略</span><span class="w"> </span><span class="err">π₁</span>
<span class="mi">2</span><span class="o">.</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">N</span><span class="o">:</span>
<span class="w">   </span><span class="n">a</span><span class="o">.</span><span class="w"> </span><span class="err">用当前策略</span><span class="w"> </span><span class="err">πᵢ</span><span class="w"> </span><span class="err">收集轨迹</span>
<span class="w">   </span><span class="n">b</span><span class="o">.</span><span class="w"> </span><span class="err">对轨迹中的每个状态</span><span class="w"> </span><span class="n">s</span><span class="err">，查询专家动作</span><span class="w"> </span><span class="n">a</span><span class="o">*</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">π</span><span class="o">*(</span><span class="n">s</span><span class="o">)</span>
<span class="w">   </span><span class="n">c</span><span class="o">.</span><span class="w"> </span><span class="err">聚合数据：</span><span class="n">Dᵢ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Dᵢ</span><span class="err">₋₁</span><span class="w"> </span><span class="err">∪</span><span class="w"> </span><span class="o">{(</span><span class="n">s</span><span class="o">,</span><span class="w"> </span><span class="n">a</span><span class="o">*)}</span>
<span class="w">   </span><span class="n">d</span><span class="o">.</span><span class="w"> </span><span class="err">在</span><span class="w"> </span><span class="n">Dᵢ</span><span class="w"> </span><span class="err">上重新训练策略</span><span class="w"> </span><span class="err">πᵢ₊₁</span>

<span class="mi">3</span><span class="o">.</span><span class="w"> </span><span class="err">返回最终策略</span><span class="w"> </span><span class="err">πₙ₊₁</span>
</code></pre></div>

<p><strong>关键改进</strong>：</p>
<ol>
<li><strong>在线数据收集</strong>：使用学习策略访问的状态，而非专家状态</li>
<li><strong>专家标注</strong>：为这些状态获取专家标签</li>
<li><strong>数据聚合</strong>：混合历史数据防止遗忘</li>
</ol>
<p><strong>理论保证</strong>：
DAgger 提供了误差界：
$$J(\pi_{DAgger}) - J(\pi^*) \leq O(\epsilon T)$$
相比行为克隆的 $O(\epsilon T^2)$，这是显著改进。</p>
<h3 id="33">3.3 实践中的改进</h3>
<p><strong>SafeDAgger</strong>：
为了安全性，在危险状态下切换到专家控制：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">safe_dagger_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">learned_policy</span><span class="p">,</span> <span class="n">expert_policy</span><span class="p">,</span> <span class="n">safety_checker</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">safety_checker</span><span class="o">.</span><span class="n">is_safe</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">learned_policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">expert_policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>  <span class="c1"># 仍然记录专家标签</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">expert_policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>  <span class="c1"># 专家接管</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">action</span>
    <span class="k">return</span> <span class="n">action</span><span class="p">,</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</code></pre></div>

<p><strong>HG-DAgger (Human-Gated DAgger)</strong>：
让人类专家决定何时介入：</p>
<ul>
<li>专家观察机器人执行</li>
<li>当偏离期望行为时，专家接管控制</li>
<li>减少标注成本，只在关键时刻查询</li>
</ul>
<p><strong>ThriftyDAgger</strong>：
选择性查询最有价值的状态：</p>
<ul>
<li>使用不确定性估计(如集成方差)</li>
<li>优先查询高不确定性状态</li>
<li>平衡探索和标注成本</li>
</ul>
<p><strong>EnsembleDAgger</strong>：
使用策略集成提高鲁棒性：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">EnsembleDAgger</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_models</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">create_model</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_models</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">models</span><span class="p">]</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">uncertainty</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span><span class="p">,</span> <span class="n">uncertainty</span>

    <span class="k">def</span> <span class="nf">should_query_expert</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">uncertainty</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">uncertainty</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">threshold</span>
</code></pre></div>

<hr />
<h2 id="4-irl">4. 逆强化学习(IRL)原理</h2>
<h3 id="41">4.1 奖励函数学习</h3>
<p>逆强化学习的核心思想是：专家的行为隐含了某个未知的奖励函数，通过观察专家演示来推断这个奖励函数。</p>
<p><strong>问题形式化</strong>：
给定：</p>
<ul>
<li>专家演示轨迹 $\{\tau_1^*, \tau_2^*, ..., \tau_n^*\}$</li>
<li>MDP中除奖励外的所有要素 $(\mathcal{S}, \mathcal{A}, P, \gamma)$</li>
</ul>
<p>目标：学习奖励函数 $R(s,a)$ 使得专家策略 $\pi^*$ 是最优的</p>
<p><strong>基础IRL算法</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="mf">1.</span><span class="w"> </span><span class="n">初始化奖励函数</span><span class="w"> </span><span class="n">R</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">重复</span><span class="err">：</span>
<span class="w">   </span><span class="n">a</span><span class="mf">.</span><span class="w"> </span><span class="n">用当前R求解最优策略</span><span class="w"> </span><span class="n">π</span>
<span class="w">   </span><span class="n">b</span><span class="mf">.</span><span class="w"> </span><span class="n">计算特征期望</span><span class="err">：</span>
<span class="w">      </span><span class="n">μ_π</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">E</span><span class="err">[</span><span class="n">Σ</span><span class="w"> </span><span class="n">γ</span><span class="o">^</span><span class="n">t</span><span class="w"> </span><span class="n">φ</span><span class="p">(</span><span class="n">s_t</span><span class="p">,</span><span class="w"> </span><span class="n">a_t</span><span class="p">)</span><span class="w"> </span><span class="err">|</span><span class="w"> </span><span class="n">π</span><span class="err">]</span>
<span class="w">      </span><span class="n">μ_E</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">E</span><span class="err">[</span><span class="n">Σ</span><span class="w"> </span><span class="n">γ</span><span class="o">^</span><span class="n">t</span><span class="w"> </span><span class="n">φ</span><span class="p">(</span><span class="n">s_t</span><span class="p">,</span><span class="w"> </span><span class="n">a_t</span><span class="p">)</span><span class="w"> </span><span class="err">|</span><span class="w"> </span><span class="n">专家演示</span><span class="err">]</span>
<span class="w">   </span><span class="n">c</span><span class="mf">.</span><span class="w"> </span><span class="n">更新奖励函数使</span><span class="w"> </span><span class="n">μ_π</span><span class="w"> </span><span class="n">接近</span><span class="w"> </span><span class="n">μ_E</span>

<span class="mf">3.</span><span class="w"> </span><span class="n">返回学习到的奖励函数</span><span class="w"> </span><span class="n">R</span>
</code></pre></div>

<p><strong>特征匹配原理</strong>：
IRL假设奖励函数可以表示为特征的线性组合：
$$R(s,a) = w^T \phi(s,a)$$
专家策略应该最大化期望奖励：
$$\pi^* = \arg\max_\pi \mathbb{E}_{\tau \sim \pi}[w^T \mu_\pi]$$</p>
<h3 id="42-irl">4.2 最大熵IRL</h3>
<p>最大熵IRL (MaxEnt IRL) 解决了奖励函数的歧义性问题：</p>
<p><strong>原理</strong>：在所有解释专家行为的奖励函数中，选择导致最大策略熵的那个。</p>
<p><strong>概率模型</strong>：
轨迹的概率正比于其累积奖励：
$$P(\tau) \propto \exp(R(\tau)) = \exp(\sum_t R(s_t, a_t))$$
<strong>目标函数</strong>：
最大化对数似然：
$$\mathcal{L} = \sum_i \log P(\tau_i^*) - \log Z$$
其中 $Z = \sum_\tau \exp(R(\tau))$ 是配分函数。</p>
<p><strong>软值迭代</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">soft_value_iteration</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_states</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">V_new</span> <span class="o">=</span> <span class="n">soft_bellman_backup</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="n">V_new</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V_new</span>

    <span class="c1"># 计算软Q函数</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">R</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">expected_V</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>

    <span class="c1"># 导出策略</span>
    <span class="n">π</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Q</span> <span class="o">-</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">π</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">Q</span>

<span class="k">def</span> <span class="nf">soft_bellman_backup</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">R</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V_next</span>  <span class="c1"># V_next是下一状态的值</span>
    <span class="k">return</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 软最大值</span>
</code></pre></div>

<h3 id="43-irl">4.3 深度IRL方法</h3>
<p><strong>神经网络奖励函数</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">NeuralRewardFunction</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">input_dim</span> <span class="o">=</span> <span class="n">state_dim</span> <span class="o">+</span> <span class="n">action_dim</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">h_dim</span> <span class="ow">in</span> <span class="n">hidden_dims</span><span class="p">:</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
            <span class="p">])</span>
            <span class="n">input_dim</span> <span class="o">=</span> <span class="n">h_dim</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>

<p><strong>Guided Cost Learning (GCL)</strong>：
结合采样和重要性权重的高效IRL算法：</p>
<ol>
<li><strong>采样阶段</strong>：从当前策略采样轨迹</li>
<li>
<p><strong>重要性采样</strong>：计算轨迹权重
$$w_i = \frac{p(\tau_i | \text{optimal})}{p(\tau_i | \text{sample policy})}$$</p>
</li>
<li>
<p><strong>奖励更新</strong>：最大化加权对数似然</p>
</li>
</ol>
<p><strong>对抗IRL (AIRL)</strong>：
使用判别器区分专家和策略轨迹：
$$D(s,a) = \frac{\exp(f_\theta(s,a))}{\exp(f_\theta(s,a)) + \pi(a|s)}$$
其中 $f_\theta$ 是学习的奖励函数。</p>
<hr />
<h2 id="5-gail">5. GAIL与对抗模仿学习</h2>
<h3 id="51">5.1 生成对抗框架</h3>
<p>Generative Adversarial Imitation Learning (GAIL) 将模仿学习转化为生成对抗问题，避免了显式的奖励函数建模。</p>
<p><strong>核心思想</strong>：</p>
<ul>
<li>生成器：策略网络 $\pi_\theta$ 生成轨迹</li>
<li>判别器：区分专家轨迹和策略轨迹</li>
<li>通过对抗训练使策略生成的轨迹与专家轨迹无法区分</li>
</ul>
<p><strong>目标函数</strong>：
$$\min_\theta \max_\omega \mathbb{E}_{\pi^*}[\log D_\omega(s,a)] + \mathbb{E}_{\pi_\theta}[\log(1-D_\omega(s,a))] - \lambda H(\pi_\theta)$$
其中 $H(\pi_\theta)$ 是策略熵，用于鼓励探索。</p>
<p><strong>与IRL的联系</strong>：
GAIL可以看作是IRL和RL的组合，但跳过了显式的奖励函数学习：</p>
<div class="codehilite"><pre><span></span><code>传统IRL+RL: 专家演示 → 奖励函数 → 最优策略
GAIL:       专家演示 → 最优策略 (通过对抗学习)
</code></pre></div>

<h3 id="52">5.2 判别器与生成器设计</h3>
<p><strong>判别器架构</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">Discriminator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">input_dim</span> <span class="o">=</span> <span class="n">state_dim</span> <span class="o">+</span> <span class="n">action_dim</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">h_dim</span> <span class="ow">in</span> <span class="n">hidden_dims</span><span class="p">:</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">h_dim</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>  <span class="c1"># Tanh通常比ReLU更稳定</span>
            <span class="p">])</span>
            <span class="n">input_dim</span> <span class="o">=</span> <span class="n">h_dim</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">compute_reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
        <span class="c1"># 用判别器输出作为奖励信号</span>
        <span class="n">D</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">D</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">D</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
</code></pre></div>

<p><strong>策略生成器</strong>：
使用任何策略梯度算法(PPO/TRPO/SAC)，将判别器输出作为奖励：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">GAILTrainer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">discriminator</span><span class="p">,</span> <span class="n">expert_dataset</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy</span> <span class="o">=</span> <span class="n">policy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">discriminator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expert_dataset</span> <span class="o">=</span> <span class="n">expert_dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_optimizer</span> <span class="o">=</span> <span class="n">PPO</span><span class="p">(</span><span class="n">policy</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># 1. 收集策略轨迹</span>
        <span class="n">policy_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collect_trajectories</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">)</span>

        <span class="c1"># 2. 采样专家数据</span>
        <span class="n">expert_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expert_dataset</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">policy_batch</span><span class="p">))</span>

        <span class="c1"># 3. 更新判别器</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_discriminator</span><span class="p">(</span><span class="n">expert_batch</span><span class="p">,</span> <span class="n">policy_batch</span><span class="p">)</span>

        <span class="c1"># 4. 计算策略奖励</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="o">.</span><span class="n">compute_reward</span><span class="p">(</span>
            <span class="n">policy_batch</span><span class="o">.</span><span class="n">states</span><span class="p">,</span> 
            <span class="n">policy_batch</span><span class="o">.</span><span class="n">actions</span>
        <span class="p">)</span>

        <span class="c1"># 5. 更新策略</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">policy_batch</span><span class="p">,</span> <span class="n">rewards</span><span class="p">)</span>
</code></pre></div>

<h3 id="53">5.3 训练稳定性技巧</h3>
<p>GAIL训练容易不稳定，需要特殊技巧：</p>
<p><strong>1. 梯度惩罚 (WGAN-GP)</strong>：
$$\mathcal{L}_D = -\mathbb{E}_{expert}[D] + \mathbb{E}_{policy}[D] + \lambda \mathbb{E}_{\hat{x}}[(||\nabla_{\hat{x}}D||_2 - 1)^2]$$
其中 $\hat{x}$ 是专家和策略数据的插值。</p>
<p><strong>2. 谱归一化</strong>：
限制判别器的Lipschitz常数：</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">torch.nn.utils</span> <span class="kn">import</span> <span class="n">spectral_norm</span>

<span class="k">class</span> <span class="nc">SpectralNormDiscriminator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">spectral_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">state_dim</span> <span class="o">+</span> <span class="n">action_dim</span><span class="p">,</span> <span class="mi">256</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">spectral_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">spectral_norm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="p">)</span>
</code></pre></div>

<p><strong>3. 缓冲区重放</strong>：
维护历史策略数据缓冲区，防止判别器过拟合当前策略：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">capacity</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">capacity</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trajectories</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">trajectories</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
</code></pre></div>

<p><strong>4. 学习率调度</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 判别器学习率应该较小</span>
<span class="n">d_lr</span> <span class="o">=</span> <span class="mf">3e-4</span>
<span class="n">g_lr</span> <span class="o">=</span> <span class="mf">3e-4</span>

<span class="c1"># 使用不同的更新频率</span>
<span class="n">d_updates_per_g_update</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># 判别器更新更频繁</span>
</code></pre></div>

<p><strong>变体算法</strong>：</p>
<p><strong>InfoGAIL</strong>：
加入互信息最大化，学习可解释的潜在码：
$$\mathcal{L} = \mathcal{L}_{GAIL} + \lambda I(c; \tau)$$
<strong>SQIL (Soft Q Imitation Learning)</strong>：
简化的离线模仿学习方法：</p>
<ul>
<li>专家数据奖励设为 r=1</li>
<li>策略数据奖励设为 r=0</li>
<li>直接用SAC/TD3等离线RL算法训练</li>
</ul>
<hr />
<h2 id="fsd">案例研究：特斯拉FSD的模仿学习架构</h2>
<h3 id="_3">系统架构概述</h3>
<p>特斯拉FSD (Full Self-Driving) 系统是工业界最大规模的模仿学习部署之一。其核心是从百万级人类驾驶数据中学习驾驶策略。</p>
<p><strong>数据规模</strong>：</p>
<ul>
<li>数据来源：全球超过100万辆特斯拉车辆</li>
<li>数据量：每天处理10PB+的驾驶数据</li>
<li>场景覆盖：各种天气、路况、交通状况</li>
</ul>
<p><strong>系统架构</strong>：</p>
<div class="codehilite"><pre><span></span><code>传感器输入 → 感知网络 → 特征融合 → 策略网络 → 控制输出
     ↑                      ↑
  8个相机              BEV特征空间
</code></pre></div>

<h3 id="_4">关键技术特点</h3>
<p><strong>1. 大规模数据收集系统</strong>：
- <strong>影子模式(Shadow Mode)</strong>：在人类驾驶时运行神经网络，记录预测与实际的差异
- <strong>触发式收集</strong>：当检测到有趣或困难场景时自动上传数据
- <strong>自动标注</strong>：利用未来帧信息自动生成训练标签</p>
<p><strong>2. 多任务学习架构</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 伪代码：多任务策略网络</span>
<span class="k">class</span> <span class="nc">FSDPolicyNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">VisionTransformer</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bev_decoder</span> <span class="o">=</span> <span class="n">BEVDecoder</span><span class="p">()</span>

        <span class="c1"># 多个输出头</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trajectory_head</span> <span class="o">=</span> <span class="n">TrajectoryHead</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">speed_head</span> <span class="o">=</span> <span class="n">SpeedHead</span><span class="p">()</span>  
        <span class="bp">self</span><span class="o">.</span><span class="n">lane_change_head</span> <span class="o">=</span> <span class="n">LaneChangeHead</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">traffic_light_head</span> <span class="o">=</span> <span class="n">TrafficLightHead</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">camera_inputs</span><span class="p">):</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span><span class="n">camera_inputs</span><span class="p">)</span>
        <span class="n">bev_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bev_decoder</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;trajectory&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">trajectory_head</span><span class="p">(</span><span class="n">bev_features</span><span class="p">),</span>
            <span class="s1">&#39;speed&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">speed_head</span><span class="p">(</span><span class="n">bev_features</span><span class="p">),</span>
            <span class="s1">&#39;lane_change&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">lane_change_head</span><span class="p">(</span><span class="n">bev_features</span><span class="p">),</span>
            <span class="s1">&#39;traffic_light&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">traffic_light_head</span><span class="p">(</span><span class="n">bev_features</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div>

<p><strong>3. 端到端学习vs模块化设计</strong>：
- 早期版本：模块化(感知→规划→控制)
- 当前趋势：端到端(传感器→动作)
- 混合方案：端到端主干+安全约束模块</p>
<h3 id="_5">处理分布偏移的策略</h3>
<p><strong>1. 主动学习循环</strong>：</p>
<div class="codehilite"><pre><span></span><code>部署 → 收集困难案例 → 人工标注 → 重训练 → 验证 → 部署
         ↑                                    ↓
         ←────────────────────────────────────
</code></pre></div>

<p><strong>2. 对抗样本挖掘</strong>：
- 识别模型失败的场景
- 生成相似的合成场景
- 增强训练数据多样性</p>
<p><strong>3. 在线适应</strong>：
- 车载模型微调(受限于算力)
- 个性化驾驶风格学习
- 持续学习防止灾难性遗忘</p>
<h3 id="_6">工程挑战与解决方案</h3>
<p><strong>延迟优化</strong>：</p>
<ul>
<li>模型量化：FP32→INT8</li>
<li>模型剪枝：去除冗余连接</li>
<li>知识蒸馏：大模型→小模型</li>
</ul>
<p><strong>安全保障</strong>：</p>
<ul>
<li>多重冗余：多个独立模型投票</li>
<li>规则约束：硬编码安全规则</li>
<li>降级策略：检测到异常时平滑降级</li>
</ul>
<hr />
<h2 id="qcql">高级话题：离线强化学习与保守Q学习(CQL)</h2>
<h3 id="_7">离线强化学习的动机</h3>
<p>离线强化学习从固定数据集学习，不需要与环境交互，这对机器人应用特别重要：</p>
<ul>
<li><strong>安全性</strong>：避免危险的探索</li>
<li><strong>数据利用</strong>：充分利用历史数据</li>
<li><strong>计算效率</strong>：可以离线批量训练</li>
</ul>
<p><strong>核心挑战</strong>：</p>
<ul>
<li><strong>外推误差</strong>：Q值在未见过的状态-动作对上的过估计</li>
<li><strong>分布偏移</strong>：数据分布与策略分布不匹配</li>
<li><strong>有限覆盖</strong>：数据可能不覆盖所有重要区域</li>
</ul>
<h3 id="qcql_1">保守Q学习(CQL)原理</h3>
<p>CQL通过学习Q函数的下界来避免过估计问题：</p>
<p><strong>CQL损失函数</strong>：
$$\mathcal{L}_{CQL}(\theta) = \alpha \mathbb{E}_{s \sim \mathcal{D}}[\log \sum_a \exp(Q_\theta(s,a)) - \mathbb{E}_{a \sim \pi_\beta(a|s)}[Q_\theta(s,a)]] + \mathcal{L}_{SAC}(\theta)$$</p>
<p>其中：</p>
<ul>
<li>第一项：推高所有动作的Q值</li>
<li>第二项：降低数据集中动作的Q值</li>
<li>结果：对OOD(out-of-distribution)动作保守估计</li>
</ul>
<p><strong>实现细节</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">CQL</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q_network</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_network</span> <span class="o">=</span> <span class="n">q_network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy</span> <span class="o">=</span> <span class="n">policy</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>

    <span class="k">def</span> <span class="nf">compute_cql_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">dones</span><span class="p">):</span>
        <span class="c1"># 标准TD损失</span>
        <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_network</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">next_q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_target_q</span><span class="p">(</span><span class="n">next_states</span><span class="p">)</span>
            <span class="n">target_q</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dones</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">next_q</span>
        <span class="n">td_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">q_values</span><span class="p">,</span> <span class="n">target_q</span><span class="p">)</span>

        <span class="c1"># CQL正则化项</span>
        <span class="c1"># 计算log-sum-exp over actions</span>
        <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">10</span>
        <span class="n">random_actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span><span class="p">)</span>
        <span class="n">random_q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_network</span><span class="p">(</span><span class="n">states</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">random_actions</span><span class="p">)</span>
        <span class="n">logsumexp_q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">random_q</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># 数据集动作的Q值</span>
        <span class="n">dataset_q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_network</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>

        <span class="c1"># CQL损失</span>
        <span class="n">cql_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">logsumexp_q</span> <span class="o">-</span> <span class="n">dataset_q</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">td_loss</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">cql_loss</span>
</code></pre></div>

<h3 id="rl">离线模仿学习与离线RL的结合</h3>
<p><strong>IQL (Implicit Q-Learning)</strong>：
避免显式策略改进，直接从数据学习：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">iql_loss</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span><span class="p">):</span>
    <span class="c1"># 学习V函数(期望Q值的分位数)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">target_q</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>
    <span class="n">v_loss</span> <span class="o">=</span> <span class="n">expectile_loss</span><span class="p">(</span><span class="n">v</span><span class="p">(</span><span class="n">states</span><span class="p">),</span> <span class="n">target_q</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

    <span class="c1"># 学习Q函数</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">next_v</span> <span class="o">=</span> <span class="n">v</span><span class="p">(</span><span class="n">next_states</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">next_v</span>
    <span class="n">q_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">q</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>

    <span class="c1"># 通过advantage加权行为克隆学习策略</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">advantage</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span> <span class="o">-</span> <span class="n">v</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">advantage</span> <span class="o">/</span> <span class="n">beta</span><span class="p">)</span>
    <span class="n">policy_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">weight</span> <span class="o">*</span> <span class="n">log_prob</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">q_loss</span> <span class="o">+</span> <span class="n">v_loss</span> <span class="o">+</span> <span class="n">policy_loss</span>
</code></pre></div>

<h3 id="_8">实践考虑</h3>
<p><strong>数据质量评估</strong>：</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">assess_dataset_quality</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;coverage&#39;</span><span class="p">:</span> <span class="n">compute_state_coverage</span><span class="p">(</span><span class="n">dataset</span><span class="p">),</span>
        <span class="s1">&#39;return_distribution&#39;</span><span class="p">:</span> <span class="n">compute_return_stats</span><span class="p">(</span><span class="n">dataset</span><span class="p">),</span>
        <span class="s1">&#39;trajectory_length&#39;</span><span class="p">:</span> <span class="n">compute_trajectory_lengths</span><span class="p">(</span><span class="n">dataset</span><span class="p">),</span>
        <span class="s1">&#39;action_diversity&#39;</span><span class="p">:</span> <span class="n">compute_action_entropy</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">metrics</span>
</code></pre></div>

<p><strong>混合在线/离线学习</strong>：</p>
<ol>
<li>先用离线数据预训练</li>
<li>在线微调with安全约束</li>
<li>定期更新离线数据集</li>
</ol>
<hr />
<h2 id="_9">本章小结</h2>
<p>本章系统介绍了模仿学习的核心方法和实践技术：</p>
<p><strong>关键概念回顾</strong>：</p>
<ol>
<li><strong>行为克隆</strong>：将模仿学习转化为监督学习，简单有效但存在分布偏移问题</li>
<li><strong>DAgger算法</strong>：通过迭代数据收集解决分布偏移，提供线性误差界</li>
<li><strong>逆强化学习</strong>：从演示中学习奖励函数，理解专家的潜在目标</li>
<li><strong>GAIL</strong>：通过对抗学习直接学习策略，避免显式奖励建模</li>
<li><strong>离线强化学习</strong>：从固定数据集学习，CQL通过保守估计避免外推误差</li>
</ol>
<p><strong>核心公式汇总</strong>：</p>
<ul>
<li>行为克隆损失：$\mathcal{L}(\theta) = \mathbb{E}_{(s,a) \sim \mathcal{D}}[\ell(\pi_\theta(s), a)]$</li>
<li>DAgger误差界：$J(\pi_{DAgger}) - J(\pi^*) \leq O(\epsilon T)$</li>
<li>最大熵IRL：$P(\tau) \propto \exp(\sum_t R(s_t, a_t))$</li>
<li>GAIL目标：$\min_\theta \max_\omega \mathbb{E}_{\pi^*}[\log D_\omega] + \mathbb{E}_{\pi_\theta}[\log(1-D_\omega)]$</li>
<li>CQL正则化：$\alpha[\log \sum_a \exp Q(s,a) - \mathbb{E}_{a \sim \pi_\beta}[Q(s,a)]]$</li>
</ul>
<p><strong>方法选择指南</strong>：</p>
<ul>
<li>数据充足+任务简单 → 行为克隆</li>
<li>可以查询专家 → DAgger</li>
<li>需要理解意图 → IRL</li>
<li>大规模复杂任务 → GAIL</li>
<li>安全性要求高 → 离线RL(CQL/IQL)</li>
</ul>
<hr />
<h2 id="_10">练习题</h2>
<h3 id="_11">基础题</h3>
<p><strong>练习15.1</strong>：分布偏移的影响
给定一个简单的1D导航任务，专家策略是 $a^* = -\text{sign}(s)$（向原点移动）。如果行为克隆的策略有误差 $\hat{a} = a^* + \epsilon$，其中 $\epsilon \sim \mathcal{N}(0, 0.01)$，计算T=100步后的期望位置偏差。</p>
<details>
<summary>提示</summary>
<p>考虑误差的累积效应，每步的误差会影响下一步的状态。</p>
</details>
<details>
<summary>答案</summary>
<p>误差累积导致期望偏差为 $O(\epsilon \sqrt{T}) \approx 0.1 \times \sqrt{100} = 1.0$。这展示了即使小误差也会随时间累积。</p>
</details>
<p><strong>练习15.2</strong>：DAgger数据聚合
假设初始数据集有1000个专家样本，每轮DAgger收集500个新样本。如果使用0.5的混合比例（50%新数据，50%历史数据），第3轮训练时的有效数据集大小是多少？</p>
<details>
<summary>提示</summary>
<p>考虑数据聚合策略和采样比例。</p>
</details>
<details>
<summary>答案</summary>
<p>总数据量：1000 + 500×3 = 2500个样本。使用50-50混合，有效训练集包含1250个历史样本和1250个最新样本的混合。</p>
</details>
<p><strong>练习15.3</strong>：GAIL判别器输出解释
如果GAIL的判别器对专家数据输出0.9，对策略数据输出0.3，这意味着什么？应该如何调整训练？</p>
<details>
<summary>提示</summary>
<p>判别器输出表示样本来自专家的概率。</p>
</details>
<details>
<summary>答案</summary>
<p>判别器能够轻易区分专家(0.9)和策略(0.3)数据，说明策略还需要改进。应该：1)增加策略更新步数，2)检查是否存在模式崩塌，3)可能需要调整学习率。</p>
</details>
<h3 id="_12">挑战题</h3>
<p><strong>练习15.4</strong>：多模态动作处理
设计一个混合密度网络(MDN)来处理十字路口左转/直行的多模态决策。网络应该输出什么？如何从输出中采样动作？</p>
<details>
<summary>提示</summary>
<p>MDN输出多个高斯分量的参数：权重、均值和方差。</p>
</details>
<details>
<summary>答案</summary>
<p>网络输出：K个分量的 {πₖ(权重), μₖ(均值), σₖ(标准差)}。采样过程：1)根据权重πₖ采样分量索引k，2)从N(μₖ, σₖ²)采样动作。对于左转/直行，K=2通常足够。</p>
</details>
<p><strong>练习15.5</strong>：IRL奖励函数设计
给定抓取任务的专家演示，设计一个包含以下特征的奖励函数：距离物体、夹爪开合度、末端速度。如何确定特征权重？</p>
<details>
<summary>提示</summary>
<p>使用最大熵IRL框架，通过特征匹配学习权重。</p>
</details>
<details>
<summary>答案</summary>
<p>奖励函数：R(s,a) = w₁·(-dist) + w₂·grip_match + w₃·(-velocity)。通过最大熵IRL学习权重：1)计算专家特征期望，2)迭代优化权重使策略特征期望匹配专家，3)使用软值迭代求解策略。</p>
</details>
<p><strong>练习15.6</strong>：离线RL数据集诊断
给定一个机器人操作数据集，设计一个诊断流程来评估其是否适合离线RL训练。应该检查哪些指标？</p>
<details>
<summary>提示</summary>
<p>考虑覆盖度、质量、多样性等多个维度。</p>
</details>
<details>
<summary>答案</summary>
<p>诊断流程：1)状态覆盖度(使用KDE估计)，2)回报分布(检查是否有高质量演示)，3)动作多样性(计算熵)，4)轨迹完整性(检查截断)，5)OOD检测(训练VAE检测异常)。根据这些指标决定是否需要数据增强或收集更多数据。</p>
</details>
<p><strong>练习15.7</strong>：安全DAgger实现
设计一个安全的DAgger变体用于真实机器人，要求：1)防止危险动作，2)最小化人工干预，3)保证学习效率。</p>
<details>
<summary>提示</summary>
<p>结合安全约束、不确定性估计和主动学习。</p>
</details>
<details>
<summary>答案</summary>
<p>实现方案：1)安全层：使用CBF(控制屏障函数)过滤危险动作，2)不确定性触发：只在模型不确定时查询专家(使用集成方差)，3)优先级队列：根据(不确定性×任务重要性)排序查询，4)回滚机制：检测到异常立即切换专家控制。</p>
</details>
<p><strong>练习15.8</strong>：GAIL训练调试
你的GAIL训练出现模式崩塌(所有轨迹相似)。列出可能的原因和对应的解决方案。</p>
<details>
<summary>提示</summary>
<p>考虑判别器过强、探索不足、奖励信号等因素。</p>
</details>
<details>
<summary>答案</summary>
<p>原因及解决方案：1)判别器过强→降低判别器学习率/容量，2)探索不足→增加策略熵正则化，3)奖励信号退化→使用梯度惩罚或谱归一化，4)数据不平衡→使用重放缓冲区，5)初始化不当→使用行为克隆预训练策略。</p>
</details>
<hr />
<h2 id="_13">常见陷阱与错误</h2>
<h3 id="1_1">1. 数据收集陷阱</h3>
<p><strong>错误</strong>：只收集成功轨迹</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 错误：偏向性数据收集</span>
<span class="k">if</span> <span class="n">task_successful</span><span class="p">:</span>
    <span class="n">dataset</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">trajectory</span><span class="p">)</span>  <span class="c1"># 只记录成功案例</span>
</code></pre></div>

<p><strong>正确</strong>：包含失败和恢复</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 正确：全面数据收集</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">trajectory</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">task_successful</span><span class="p">:</span>
    <span class="n">dataset</span><span class="o">.</span><span class="n">add_recovery_demo</span><span class="p">()</span>  <span class="c1"># 添加恢复演示</span>
</code></pre></div>

<h3 id="2">2. 时序忽视</h3>
<p><strong>错误</strong>：独立处理每个时间步</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 错误：忽略历史</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">current_state</span><span class="p">)</span>
</code></pre></div>

<p><strong>正确</strong>：考虑时序依赖</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 正确：包含历史</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">current_state</span><span class="p">,</span> <span class="n">history</span><span class="p">[</span><span class="o">-</span><span class="n">k</span><span class="p">:])</span>
</code></pre></div>

<h3 id="3">3. 分布偏移处理不当</h3>
<p><strong>错误</strong>：训练后直接部署</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 错误：忽视分布偏移</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">deploy_to_robot</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>  <span class="c1"># 危险！</span>
</code></pre></div>

<p><strong>正确</strong>：渐进式部署</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 正确：安全部署</span>
<span class="k">for</span> <span class="n">confidence_threshold</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]:</span>
    <span class="n">deploy_with_safety_net</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">confidence_threshold</span><span class="p">)</span>
    <span class="n">collect_failure_cases</span><span class="p">()</span>
    <span class="n">retrain_model</span><span class="p">()</span>
</code></pre></div>

<h3 id="4-gail">4. GAIL训练不稳定</h3>
<p><strong>错误</strong>：固定超参数</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 错误：不adaptive</span>
<span class="n">d_optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">g_optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</code></pre></div>

<p><strong>正确</strong>：自适应调整</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 正确：动态调整</span>
<span class="k">if</span> <span class="n">discriminator_accuracy</span> <span class="o">&gt;</span> <span class="mf">0.8</span><span class="p">:</span>
    <span class="n">d_lr</span> <span class="o">*=</span> <span class="mf">0.5</span>  <span class="c1"># 降低判别器学习率</span>
<span class="k">if</span> <span class="n">generator_loss</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
    <span class="n">g_lr</span> <span class="o">*=</span> <span class="mf">1.1</span>  <span class="c1"># 提高生成器学习率</span>
</code></pre></div>

<h3 id="5-rl">5. 离线RL外推问题</h3>
<p><strong>错误</strong>：直接最大化Q值</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 错误：Q值过估计</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">action_space</span><span class="p">)</span>
</code></pre></div>

<p><strong>正确</strong>：保守估计</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 正确：CQL保守策略</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">sample_from_policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="k">if</span> <span class="n">Q</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">conservative_threshold</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">closest_dataset_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
</code></pre></div>

<hr />
<h2 id="_14">最佳实践检查清单</h2>
<h3 id="_15">数据收集阶段</h3>
<ul>
<li>[ ] 数据集包含多样化的场景和条件</li>
<li>[ ] 包含失败案例和恢复演示</li>
<li>[ ] 多模态数据正确同步(时间戳对齐)</li>
<li>[ ] 数据质量指标计算完成</li>
<li>[ ] 隐私和安全考虑已处理</li>
</ul>
<h3 id="_16">模型设计阶段</h3>
<ul>
<li>[ ] 选择适合任务复杂度的架构</li>
<li>[ ] 考虑了多模态动作的可能性</li>
<li>[ ] 包含适当的正则化(dropout, weight decay)</li>
<li>[ ] 设计了合理的损失函数</li>
<li>[ ] 考虑了时序依赖性</li>
</ul>
<h3 id="_17">训练阶段</h3>
<ul>
<li>[ ] 实现了数据增强策略</li>
<li>[ ] 监控训练/验证损失曲线</li>
<li>[ ] 检查分布偏移指标</li>
<li>[ ] 定期保存检查点</li>
<li>[ ] 实现了早停机制</li>
</ul>
<h3 id="_18">评估阶段</h3>
<ul>
<li>[ ] 在多个测试场景评估</li>
<li>[ ] 计算任务成功率和效率指标</li>
<li>[ ] 分析失败模式</li>
<li>[ ] 与基线方法对比</li>
<li>[ ] 进行消融实验</li>
</ul>
<h3 id="_19">部署阶段</h3>
<ul>
<li>[ ] 实现安全约束和失效保护</li>
<li>[ ] 设置置信度阈值</li>
<li>[ ] 准备回滚方案</li>
<li>[ ] 建立监控和日志系统</li>
<li>[ ] 制定增量部署计划</li>
</ul>
<h3 id="_20">持续改进</h3>
<ul>
<li>[ ] 建立数据飞轮(部署→收集→改进)</li>
<li>[ ] 实现在线适应机制</li>
<li>[ ] 定期重新评估和更新</li>
<li>[ ] 记录和分析边缘案例</li>
<li>[ ] 保持与最新研究同步</li>
</ul>
            </article>
            
            <nav class="page-nav"><a href="chapter14.html" class="nav-link prev">← 第14章：灵巧操作与双臂协调</a><a href="chapter16.html" class="nav-link next">第16章：扩散模型在机器人中的应用 →</a></nav>
        </main>
    </div>
</body>
</html>