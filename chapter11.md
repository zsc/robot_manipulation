# 第11章：视觉感知基础

本章深入探讨机器人视觉感知的核心技术，从RGB-D相机的工作原理到多视角几何重建。我们将重点关注工程实践中的关键问题：如何选择合适的深度传感器、如何处理标定误差、如何在实时性和精度之间权衡。通过本章学习，读者将掌握构建鲁棒视觉系统的核心技术，理解不同传感器模态的优劣，并能够针对具体任务选择最优的感知方案。

## 11.1 RGB-D相机原理与工程选择

### 11.1.1 深度获取技术对比

现代机器人系统中常见的深度获取技术包括结构光、飞行时间(ToF)和立体视觉三种主要方案。每种技术都有其独特的物理原理和工程权衡。选择合适的深度传感器是机器人感知系统设计的第一步，直接影响后续所有视觉算法的性能上限。

**结构光技术**基于三角测量原理。投影器发射已知模式（通常是红外散斑或编码条纹），相机捕获变形后的图案，通过三角测量计算深度：

$$z = \frac{fb}{d}$$

其中 $f$ 是焦距，$b$ 是基线长度，$d$ 是视差。结构光的优势在于近距离精度高（毫米级），但在强光环境和远距离场景下性能下降。Intel RealSense D400系列采用主动立体视觉，结合了结构光投影器增强纹理贫乏区域的性能。

结构光系统的核心组件包括：
- **红外投影器**：典型功率0.5-2W，波长850nm或940nm
- **红外相机**：带通滤波器匹配投影波长，抑制环境光
- **图案编码**：随机散斑（Kinect v1）、时间编码（工业系统）或空间编码（单帧重建）

工程实现中的关键考量：
1. **基线设计**：较大基线提高精度但增加遮挡，典型值50-75mm
2. **投影图案密度**：更密集的特征点提高空间分辨率但增加匹配歧义
3. **曝光同步**：投影器与相机的精确同步避免运动模糊
4. **散热设计**：连续工作时投影器温升可达20°C，影响波长稳定性

**ToF技术**通过测量光脉冲的往返时间计算距离：

$$d = \frac{c \cdot \Delta t}{2}$$

其中 $c$ 是光速，$\Delta t$ 是飞行时间。连续波调制ToF使用相位差测量：

$$d = \frac{c \cdot \phi}{4\pi f_{mod}}$$

ToF相机如微软Azure Kinect在中远距离（0.5-5m）表现优异，对环境光鲁棒，但存在多径干扰和运动模糊问题。功耗通常在5-10W，高于结构光方案。

ToF技术的深层原理：
- **调制方式**：正弦波调制（高精度）vs 方波调制（易实现）
- **相位解缠**：多频测量解决2π模糊，典型使用3-4个频率
- **像素架构**：单抽头（顺序采样）vs 双抽头（差分测量）
- **光源设计**：VCSEL阵列提供均匀照明，边缘发射激光器提供高功率

多径干扰(MPI)是ToF的主要误差源，表现为：
$$I_{measured} = I_{direct} + \sum_i I_{indirect,i}$$

缓解策略包括：
- 偏振滤波：利用直接/间接反射的偏振差异
- 频域分析：不同路径产生不同频率响应
- 深度学习：端到端MPI补偿网络

**立体视觉技术**模拟人眼双目视差原理：

立体视觉的物理基础是三角测量，但实际工程化面临诸多挑战：
- **对应点匹配**：纹理贫乏区域的歧义性
- **遮挡处理**：左右视图的可见性差异
- **基线权衡**：精度vs共视区域的矛盾
- **标定稳定性**：机械形变和温度漂移的影响

主动立体视觉通过添加纹理投影器改善匹配：
```
被动立体：依赖场景自然纹理
    优势：无功耗、无干扰、室外可用
    劣势：白墙等场景失效
    
主动立体：投影人工纹理
    优势：全场景可用、精度可控
    劣势：功耗增加、多机干扰
```

### 11.1.2 传感器噪声模型与误差分析

深度测量的不确定性随距离呈二次增长。对于基线为 $b$ 的立体系统，深度不确定性为：

$$\sigma_z = \frac{z^2}{fb} \sigma_d$$

其中 $\sigma_d$ 是视差测量误差（典型值0.1-0.5像素）。这解释了为什么结构光相机在2米外精度急剧下降。

**深度相机的综合噪声模型**：

实际深度测量包含多种噪声源的叠加：
$$z_{measured} = z_{true} + n_{shot} + n_{thermal} + n_{quantization} + bias_{systematic}$$

各噪声分量的特性：
1. **散粒噪声** $n_{shot}$：光子到达的泊松分布，与信号强度平方根成正比
2. **热噪声** $n_{thermal}$：传感器暗电流，温度每升高6°C翻倍
3. **量化噪声** $n_{quantization}$：ADC分辨率限制，典型10-12位
4. **系统偏差** $bias_{systematic}$：标定误差、光学畸变、装配偏差

**误差传播分析**：

从原始测量到3D点云的误差传播链：
```
原始传感器 → 预处理 → 深度计算 → 坐标变换 → 点云
    ±0.1%      ×1.2      ×(z²/fb)     ×1.1       最终误差
```

考虑级联效应，1米处的典型误差分布：
- 结构光：均值0mm，标准差2-5mm
- ToF：均值5-10mm（系统偏差），标准差5-10mm  
- 立体视觉：均值0mm，标准差5-15mm（依赖纹理）

系统误差源包括：
- **温度漂移**：相机预热导致基线变化，典型漂移0.1-0.3mm/°C
- **时间同步**：RGB和深度帧的时间偏移，运动场景下产生配准误差
- **边缘噪声**：深度不连续处的"飞点"，需要通过边缘感知滤波器处理
- **材质相关误差**：透明、高反射表面导致的测量失效

**温度补偿模型**：

基线随温度的变化可建模为：
$$b(T) = b_0(1 + \alpha \Delta T)$$

其中 $\alpha$ 是热膨胀系数（铝合金：23×10⁻⁶/°C）。对于60mm基线，10°C温差产生14μm变化，在2m距离造成约5mm深度误差。

补偿策略：
1. **被动补偿**：使用因瓦合金（α<1×10⁻⁶/°C）制造基线结构
2. **主动补偿**：温度传感器+查找表修正
3. **在线标定**：基于场景特征的自适应修正

**边缘效应与飞点**：

深度不连续处的测量误差机理：
- 混合像素：前景/背景信号叠加
- 遮挡边界：立体匹配的半遮挡区域
- 散射效应：红外光在边缘的次表面散射

飞点滤波算法：
```python
# 伪代码：基于局部一致性的飞点检测
for each pixel (x,y):
    z = depth[x,y]
    neighbors = get_8_neighbors(x,y)
    z_median = median(neighbors)
    if |z - z_median| > threshold * z_median:
        mark_as_invalid(x,y)
```

### 11.1.3 硬件同步与时间戳对齐

多传感器系统中，硬件级时间同步至关重要。考虑机械臂末端速度100mm/s，10ms的时间偏差就会产生1mm的配准误差。

**时间同步的层次架构**：

```
应用层     ：任务规划时序（100ms级）
    ↓
算法层     ：感知-规划-控制循环（10ms级）
    ↓  
驱动层     ：传感器数据采集（1ms级）
    ↓
硬件层     ：触发信号/时钟同步（μs级）
```

**硬件同步方案对比**：

| 方案 | 精度 | 复杂度 | 成本 | 适用场景 |
|------|------|--------|------|----------|
| 软件时间戳 | 10-100ms | 低 | 低 | 静态场景 |
| USB同步 | 1-10ms | 中 | 低 | 低速运动 |
| GPIO触发 | 10-100μs | 中 | 中 | 通用 |
| PTP/IEEE1588 | 100ns | 高 | 高 | 高精度 |
| GPS+PPS | 100ns | 高 | 高 | 室外/多机 |

**触发模式设计**：

1. **主从触发**：单一主时钟，其他传感器从属
   ```
   主时钟 → 触发脉冲 → [相机1, 相机2, IMU, ...]
   ```
   优势：简单可靠；劣势：单点故障

2. **分布式同步**：各传感器独立但同步到公共时基
   ```
   GPS/NTP → [本地时钟1, 本地时钟2, ...]
            ↓
          传感器触发
   ```
   优势：可扩展；劣势：复杂度高

3. **级联触发**：传感器串行触发
   ```
   触发源 → 相机1 → 相机2 → 相机3
   ```
   优势：减少GPIO需求；劣势：累积延迟

实现精确同步的策略：
1. **硬件触发**：使用GPIO触发线，同步精度可达微秒级
2. **PTP协议**：IEEE 1588精密时间协议，网络同步精度100ns
3. **软件补偿**：基于运动模型的时间戳插值

```
相机1 ──┐
        ├── 触发控制器 ──> 时间戳服务器
相机2 ──┘
```

## 11.2 立体视觉与深度估计

### 11.2.1 立体匹配算法工程实现

立体匹配的核心是对应点搜索。经典的Semi-Global Matching (SGM)算法通过多方向动态规划聚合匹配代价：

$$L_r(p,d) = C(p,d) + \min \begin{cases}
L_r(p-r,d) \\
L_r(p-r,d-1) + P_1 \\
L_r(p-r,d+1) + P_1 \\
\min_i L_r(p-r,i) + P_2
\end{cases}$$

其中 $C(p,d)$ 是匹配代价，$P_1, P_2$ 是平滑惩罚项。SGM在嵌入式平台（如NVIDIA Jetson）上可实现30fps的VGA分辨率处理。

**匹配代价函数的选择**：

不同代价函数对光照变化和噪声的鲁棒性差异显著：

1. **绝对差值(SAD)**：
   $$C_{SAD} = \sum_{(i,j) \in W} |I_L(x+i,y+j) - I_R(x-d+i,y+j)|$$
   计算简单但对光照敏感

2. **归一化互相关(NCC)**：
   $$C_{NCC} = \frac{\sum (I_L - \bar{I_L})(I_R - \bar{I_R})}{\sqrt{\sum (I_L - \bar{I_L})^2 \sum (I_R - \bar{I_R})^2}}$$
   光照不变但计算复杂

3. **Census变换**：
   $$C_{Census} = Hamming(CT_L(x,y), CT_R(x-d,y))$$
   其中CT是中心像素与邻域的大小关系编码，对光照极其鲁棒

4. **混合代价**：
   $$C_{hybrid} = \alpha \cdot C_{SAD} + (1-\alpha) \cdot C_{Census}$$
   平衡效率和鲁棒性

**SGM的工程优化**：

原始SGM需要大量内存（W×H×D×8，D是视差范围），优化策略：

1. **路径压缩**：只保存当前和前一扫描线
   ```
   内存需求：O(W×D×2) 而非 O(W×H×D×8)
   ```

2. **并行化策略**：
   - 水平路径：逐行并行
   - 垂直路径：逐列并行  
   - 对角路径：波前并行
   ```
   // CUDA kernel伪代码
   __global__ void sgm_horizontal(cost_volume, L_current, L_previous) {
       int y = blockIdx.x;
       int x = threadIdx.x;
       // 每个线程处理一个像素的所有视差
       for(int d = 0; d < MAX_DISP; d++) {
           L_current[y][x][d] = compute_path_cost(...);
       }
   }
   ```

3. **视差范围预测**：
   利用低分辨率快速估计缩小搜索范围
   ```
   1/4分辨率 → 粗视差 → 全分辨率精细搜索
      5ms         2ms         15ms
   ```

**实时性能基准**（NVIDIA Jetson Xavier NX）：

| 算法 | 分辨率 | 视差级别 | FPS | 功耗 |
|------|--------|----------|-----|------|
| Block Matching | 640×480 | 64 | 120 | 5W |
| SGM (CPU) | 640×480 | 64 | 8 | 10W |
| SGM (GPU) | 640×480 | 64 | 60 | 15W |
| PSMNet | 640×480 | 192 | 10 | 20W |

### 11.2.2 亚像素精度与置信度估计

亚像素视差通过抛物线拟合实现：

$$d_{sub} = d + \frac{C(d-1) - C(d+1)}{2(C(d-1) - 2C(d) + C(d+1))}$$

置信度评估对下游任务至关重要。有效的置信度指标包括：
- **峰值比率**：最优/次优匹配代价比
- **左右一致性检查**：$|d_L(x,y) - d_R(x-d_L,y)| < \tau$
- **纹理度量**：局部梯度幅值

### 11.2.3 GPU加速与实时优化

现代立体匹配算法充分利用GPU并行性。以下是CUDA实现的关键优化：

1. **共享内存使用**：缓存匹配窗口，减少全局内存访问
2. **纹理内存**：利用2D空间局部性加速图像访问
3. **动态并行**：自适应调整线程块大小

典型性能指标（RTX 3080）：
- 640×480@120fps（32视差级别）
- 1280×720@60fps（64视差级别）
- 1920×1080@30fps（128视差级别）

## 11.3 相机标定与畸变矫正

### 11.3.1 内参标定的数值稳定性

相机内参矩阵：

$$K = \begin{bmatrix}
f_x & s & c_x \\
0 & f_y & c_y \\
0 & 0 & 1
\end{bmatrix}$$

张正友标定法通过最小化重投影误差求解：

$$\min_{K,R_i,t_i} \sum_{i,j} ||m_{ij} - \pi(K, R_i, t_i, M_j)||^2$$

**标定的数值条件数分析**：

标定问题的条件数决定了参数估计的稳定性：
$$\kappa(A) = ||A|| \cdot ||A^{-1}|| = \frac{\sigma_{max}}{\sigma_{min}}$$

条件数过大（>10³）表明问题病态，微小扰动导致解的巨大变化。改善策略：

1. **数据归一化**：将图像坐标归一化到[-1,1]
   $$T = \begin{bmatrix}
   2/w & 0 & -1 \\
   0 & 2/h & -1 \\
   0 & 0 & 1
   \end{bmatrix}$$

2. **参数解耦**：分步求解减少耦合
   - 第1步：线性求解单应矩阵
   - 第2步：分解得到内参初值
   - 第3步：非线性优化精化

3. **正则化约束**：
   $$\min_{K,R_i,t_i} \sum_{i,j} ||m_{ij} - \pi(K, R_i, t_i, M_j)||^2 + \lambda ||K - K_{prior}||^2$$

标定精度受以下因素影响：
- **标定板覆盖范围**：应覆盖80%以上视野
- **姿态多样性**：至少20个不同视角，旋转角度>30°
- **焦距初值**：错误初值导致局部最优，建议从EXIF读取

**标定板设计考量**：

| 标定板类型 | 检测精度 | 鲁棒性 | 适用场景 |
|-----------|---------|--------|----------|
| 棋盘格 | 0.1像素 | 中 | 实验室 |
| 圆形标记 | 0.05像素 | 高 | 通用 |
| ArUco | 0.2像素 | 很高 | 现场标定 |
| ChArUco | 0.1像素 | 很高 | 高精度 |

**标定质量评估指标**：

1. **重投影误差分布**：
   - RMS < 0.5像素：优秀
   - RMS < 1.0像素：可接受
   - 检查误差空间分布的均匀性

2. **参数不确定性**：
   协方差矩阵的对角元素表示参数标准差
   $$\Sigma = (J^T J)^{-1} \sigma^2$$
   其中J是雅可比矩阵

3. **交叉验证**：
   留一法验证，每次去除一张图像重新标定

### 11.3.2 畸变模型选择与矫正

径向畸变模型：

$$\begin{align}
x_d &= x_u(1 + k_1r^2 + k_2r^4 + k_3r^6) \\
y_d &= y_u(1 + k_1r^2 + k_2r^4 + k_3r^6)
\end{align}$$

切向畸变（装配误差）：

$$\begin{align}
x_d &= x_u + 2p_1x_uy_u + p_2(r^2 + 2x_u^2) \\
y_d &= y_u + p_1(r^2 + 2y_u^2) + 2p_2x_uy_u
\end{align}$$

鱼眼镜头需要等距投影模型：

$$\theta = \arctan(r/f), \quad r_d = f \cdot \theta(1 + k_1\theta^2 + k_2\theta^4)$$

### 11.3.3 在线标定与自标定

机器人运行中的标定漂移不可避免（温度、振动、老化）。在线标定策略：

1. **基于运动的自标定**：利用连续帧间的几何约束
2. **基于场景的标定**：检测已知几何特征（直线、平面）
3. **主动标定**：控制机器人运动获得最优标定轨迹

漂移检测指标：
- 重投影误差增长>0.5像素
- 深度图与点云配准残差>5mm
- 连续帧基础矩阵估计不一致

## 11.4 多视角几何与三维重建

### 11.4.1 基础矩阵与本质矩阵

对于标定相机，本质矩阵编码了相机间的相对位姿：

$$E = [t]_\times R$$

其中 $[t]_\times$ 是平移向量的反对称矩阵。本质矩阵满足：

$$x_2^T E x_1 = 0$$

五点算法是最小配置解，但八点算法在有噪声时更稳定。RANSAC框架下的鲁棒估计：

```
迭代次数 N = log(1-p) / log(1-w^s)
```

其中 $p$ 是成功概率（通常0.99），$w$ 是内点比例，$s$ 是最小集大小。

### 11.4.2 增量式SfM与全局优化

Structure from Motion管线的关键步骤：

1. **初始化**：选择基线充分的图像对（视差>30像素）
2. **三角化**：最小化重投影误差，剔除夹角<2°的点
3. **PnP求解**：EPnP或P3P+RANSAC估计新视角
4. **光束平差(BA)**：联合优化相机位姿和3D点

大规模BA的效率优化：
- **稀疏性利用**：Schur补技术降维
- **增量式求解**：仅优化新加入的变量
- **局部BA**：共视图内的局部优化

### 11.4.3 稠密重建与表面生成

多视角立体(MVS)从标定图像恢复稠密深度。PatchMatch算法通过随机搜索和传播加速：

```
初始化: 随机深度和法向量
迭代:
  空间传播: 从邻域继承好的假设
  视角传播: 从其他视图传播
  随机扰动: 局部精化
```

深度图融合使用TSDF（截断符号距离场）：

$$TSDF(x) = \min(\max(\frac{d_{proj} - d_{obs}}{\delta}, -1), 1)$$

Marching Cubes提取等值面生成三角网格。泊松重建通过求解：

$$\Delta \chi = \nabla \cdot \vec{V}$$

获得更平滑的表面，其中 $\vec{V}$ 是定向点云的向量场。

## 11.5 图像特征提取：传统vs深度学习

### 11.5.1 经典特征的工程价值

尽管深度学习主导了视觉任务，经典特征在特定场景仍有优势：

**SIFT特征**具有尺度和旋转不变性：
- 计算复杂度：O(n·m·s)，n是像素数，m是方向数，s是尺度数
- 128维描述子，浮点计算约20ms/帧(640×480)
- 专利到期(2020年)，可自由使用

**ORB特征**为实时应用优化：
- FAST角点检测 + BRIEF描述子
- 二进制描述子，汉明距离匹配
- 计算速度：约3ms/帧，适合嵌入式平台

### 11.5.2 深度特征的效率与泛化

SuperPoint等学习特征展现卓越性能：
- 自监督训练，合成数据预训练
- 端到端可微，联合优化检测和描述
- 推理速度：15ms/帧(GPU)，50ms/帧(CPU)

特征匹配的现代方法：
- **SuperGlue**：图神经网络的注意力机制
- **LoFTR**：无需显式特征检测的稠密匹配
- **LightGlue**：轻量化版本，速度提升5倍

### 11.5.3 特征选择的工程考量

选择特征时的关键因素：

1. **重复性**：相同场景不同视角的检测一致性
2. **区分性**：描述子的匹配正确率
3. **效率**：计算和匹配速度
4. **内存占用**：描述子维度和存储需求

基准测试（HPatches数据集）：
| 方法 | mAP@3px | 速度(ms) | 内存(MB) |
|------|---------|----------|----------|
| SIFT | 45.2 | 20 | 1.5 |
| ORB | 38.7 | 3 | 0.5 |
| SuperPoint | 68.3 | 15 | 45 |
| DISK | 71.2 | 18 | 52 |

## 11.6 案例研究：Intel RealSense在机器人抓取中的应用

### 11.6.1 系统架构与集成

以Amazon拣选机器人为例，分析RealSense D435i的集成方案：

**硬件配置**：
- D435i主动立体深度 + IMU
- 工作距离：0.3-3m，精度<2%@2m
- 视场角：87°×58°（深度），69°×42°（RGB）
- 帧率：30fps@1280×720（深度+RGB同步）

**机械集成要点**：
1. **安装位置**：手眼标定决定eye-in-hand vs eye-to-hand
2. **振动隔离**：橡胶减震垫降低机械臂运动干扰
3. **线缆管理**：USB3.0屏蔽线缆，避免电机EMI干扰
4. **散热设计**：主动风冷，防止热漂移>5°C

### 11.6.2 实时点云处理管线

高效的点云处理流程：

```
深度图 → 点云生成 → 滤波 → 分割 → 配准 → 抓取规划
  ↓         ↓          ↓       ↓        ↓         ↓
 30ms     5ms        3ms     8ms      12ms      15ms
```

**关键优化**：
1. **SSE/AVX向量化**：点云转换加速4倍
2. **空间索引**：KD-tree或Octree加速邻域搜索
3. **GPU管线**：CUDA点云滤波，PCL GPU模块
4. **增量处理**：仅更新变化区域

### 11.6.3 抓取点检测与质量评估

基于点云的抓取检测流程：

1. **平面分割**：RANSAC检测工作台，去除背景
2. **物体分割**：欧氏聚类或区域生长
3. **抓取采样**：Antipodal点对生成
4. **质量评估**：
   $$Q = w_1 \cdot \epsilon_{force} + w_2 \cdot \epsilon_{torque} + w_3 \cdot \epsilon_{contact}$$

实际系统性能：
- 检测成功率：95%（单一物体），85%（杂乱场景）
- 处理延迟：<100ms（从图像到抓取位姿）
- 抓取成功率：92%（已知物体），78%（未知物体）

## 11.7 高级话题：事件相机与动态视觉传感器

### 11.7.1 事件相机工作原理

事件相机（DVS）异步检测亮度变化：

$$\Delta L = L(t) - L(t - \Delta t) > C \cdot L(t - \Delta t)$$

产生事件：$e = (x, y, t, p)$，其中$p \in \{-1, +1\}$表示亮度增减。

**核心优势**：
- 时间分辨率：微秒级（vs 传统相机33ms）
- 动态范围：140dB（vs 60dB）
- 功耗：10mW（vs 3W）
- 无运动模糊

### 11.7.2 事件流处理算法

事件数据的稀疏性和异步性需要特殊处理：

1. **时间表面**：记录每个像素最新事件时间
2. **事件帧累积**：固定时间窗口内的事件计数
3. **光流估计**：局部平面拟合
   $$\frac{\partial I}{\partial t} + \nabla I \cdot v = 0$$

### 11.7.3 机器人应用场景

事件相机在高动态场景表现卓越：

- **无人机避障**：高速飞行中的障碍检测
- **机械臂跟踪**：乒乓球接球等高速任务
- **SLAM**：Event-based Visual-Inertial Odometry
- **触觉感知**：基于视觉的接触检测

集成挑战：
- 算法生态不成熟，缺乏标准工具链
- 与传统视觉融合的时间同步
- 深度学习模型的事件数据表示

## 本章小结

本章系统介绍了机器人视觉感知的基础技术：

**核心概念**：
- RGB-D相机三种深度获取原理及工程权衡
- 立体匹配的算法优化与GPU加速
- 相机标定的数值稳定性与在线校正
- 多视角几何的增量重建与全局优化
- 特征提取的效率与泛化权衡

**关键公式**：
- 三角测量：$z = \frac{fb}{d}$
- 深度不确定性：$\sigma_z = \frac{z^2}{fb} \sigma_d$
- 本质矩阵约束：$x_2^T E x_1 = 0$
- TSDF融合：$TSDF(x) = \min(\max(\frac{d_{proj} - d_{obs}}{\delta}, -1), 1)$

**工程要点**：
- 硬件同步精度直接影响配准误差
- 置信度估计对下游任务至关重要
- 实时性要求下的算法-精度权衡
- 温度漂移和振动对标定的持续影响

## 练习题

### 基础题

**11.1** 某立体相机系统基线长度60mm，焦距6mm（600像素），在2米距离处测量一个物体。如果视差测量误差为0.3像素，计算深度测量的标准差。

<details>
<summary>答案</summary>

根据深度不确定性公式：
$$\sigma_z = \frac{z^2}{fb} \sigma_d = \frac{2000^2}{600 \times 60} \times 0.3 = \frac{4000000}{36000} \times 0.3 = 33.3 \text{mm}$$

深度测量标准差约为33.3mm。
</details>

**11.2** 解释为什么ToF相机在测量黑色物体时性能下降，而结构光相机在测量白色墙面时可能失效。

<details>
<summary>答案</summary>

ToF相机：黑色物体吸收大部分红外光，反射信号弱，信噪比降低，导致测量噪声增大或完全失效。

结构光相机：白色墙面缺乏纹理，投影的散斑图案是唯一可用特征。但如果多个相机投影重叠或环境中有其他红外源，会造成图案混淆，无法正确匹配。
</details>

**11.3** 相机标定时采集了20张棋盘格图像，重投影误差RMS为0.8像素。如果要达到0.3像素的精度，应该采取哪些改进措施？

<details>
<summary>答案</summary>

1. 增加标定图像数量至30-40张
2. 确保棋盘格覆盖整个视场，特别是边角区域
3. 增加姿态多样性，包括大角度倾斜
4. 使用更高精度的标定板（如环形标记）
5. 优化光照条件，避免过曝或欠曝
6. 使用亚像素角点检测
7. 考虑高阶畸变模型（k3, k4项）
</details>

### 挑战题

**11.4** 设计一个多相机系统的时间同步方案，要求同步精度达到100微秒，相机通过千兆以太网连接。描述硬件架构和软件实现。

<details>
<summary>答案</summary>

硬件架构：
1. 使用支持IEEE 1588 PTP的网络交换机
2. 各相机配备支持硬件时间戳的网卡
3. GPS/北斗授时模块作为主时钟源
4. 触发信号通过GPIO分发

软件实现：
1. PTP守护进程同步系统时钟
2. 相机驱动读取硬件时间戳
3. 触发信号产生精确时间戳
4. 后处理时间戳对齐，插值补偿残余误差

同步流程：
- PTP持续同步，精度~100ns
- 硬件触发确保曝光同步
- 软件层时间戳关联
- 运动补偿处理残余误差
</details>

**11.5** 某机器人需要在1米距离处达到1mm的深度精度。比较不同深度相机方案（结构光、ToF、立体视觉）的可行性，给出传感器参数要求。

<details>
<summary>答案</summary>

立体视觉：
- 需要基线×焦距乘积：$bf = \frac{z^2 \cdot \sigma_d}{\sigma_z} = \frac{1000^2 \times 0.2}{1} = 200000$
- 若焦距1000像素，需基线200mm
- 视差精度需达0.2像素（亚像素）

结构光：
- Intel D435在1m处精度约1%（10mm）
- 需要定制高精度投影器
- 增加投影密度，使用相移法

ToF：
- 相位测量精度：$\sigma_\phi = \frac{4\pi f_{mod}}{c} \sigma_d = \frac{4\pi \times 30MHz}{3×10^8} \times 0.001 = 0.25mrad$
- 需要高调制频率（>50MHz）
- 多频测量减少相位模糊

推荐：高精度立体视觉+结构光增强
</details>

**11.6** 事件相机输出事件流速率为1M events/s。设计一个实时处理系统，在Jetson平台上实现光流估计，延迟<10ms。

<details>
<summary>答案</summary>

系统设计：
1. 事件缓冲：环形缓冲区，大小10k事件
2. 时间切片：1ms窗口，约1000事件/批
3. 空间分箱：16×16像素块并行处理
4. GPU加速：CUDA kernel处理每个块

算法优化：
- 局部平面拟合（3×3邻域）
- 查找表加速指数运算
- 共享内存缓存时间表面
- Warp级并行primitive

性能指标：
- 事件到达→缓冲：<0.1ms
- 批处理→GPU：<2ms  
- 光流计算：<5ms
- 结果输出：<2ms
- 总延迟：<10ms

内存需求：
- 时间表面：640×480×4B = 1.2MB
- 事件缓冲：10k×8B = 80KB
- 光流输出：640×480×8B = 2.4MB
</details>

## 常见陷阱与错误

1. **深度相机盲区**
   - 错误：忽视最小工作距离（通常20-30cm）
   - 正确：为近距离任务选择微型ToF或单目深度估计

2. **标定过拟合**
   - 错误：只在中心区域采集标定图像
   - 正确：覆盖全视场，包括畸变严重的边缘

3. **时间戳混淆**
   - 错误：使用软件接收时间作为图像时间戳
   - 正确：读取硬件曝光中点时间戳

4. **深度-RGB配准**
   - 错误：假设深度和RGB完全对齐
   - 正确：考虑视差和遮挡，使用厂商配准函数

5. **环境干扰**
   - 错误：多个机器人使用相同频率ToF
   - 正确：频率/相位调制避免串扰

6. **GPU内存溢出**
   - 错误：全分辨率点云直接处理
   - 正确：分层处理，体素降采样

## 最佳实践检查清单

### 传感器选择
- [ ] 工作距离范围满足任务需求
- [ ] 精度规格包含温度和距离因素
- [ ] 帧率满足机器人控制频率
- [ ] 视场角覆盖工作空间
- [ ] 环境光鲁棒性经过测试

### 系统集成
- [ ] 硬件触发同步已配置
- [ ] 时间戳来源明确且一致
- [ ] 振动隔离措施到位
- [ ] 热管理方案已实施
- [ ] EMI屏蔽和接地正确

### 标定维护
- [ ] 标定流程文档化
- [ ] 在线标定检查机制
- [ ] 标定参数版本管理
- [ ] 温度补偿模型建立
- [ ] 定期重标定计划

### 算法优化
- [ ] 关键路径GPU加速
- [ ] 内存访问模式优化
- [ ] 并行度充分利用
- [ ] 精度-速度权衡明确
- [ ] 异常处理完备

### 数据质量
- [ ] 置信度阈值合理设置
- [ ] 滤波参数经过调优
- [ ] 边缘和噪点处理
- [ ] 动态场景运动补偿
- [ ] 多传感器融合策略